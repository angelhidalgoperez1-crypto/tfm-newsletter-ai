{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd397238",
   "metadata": {},
   "source": [
    "# AI-powered Newsletter Generator\n",
    "**TFM – End-to-end pipeline**\n",
    "\n",
    "Este notebook implementa un pipeline completo para:\n",
    "- Agregar noticias tecnológicas de múltiples fuentes\n",
    "- Representarlas semánticamente mediante embeddings\n",
    "- Agruparlas automáticamente por áreas temáticas\n",
    "- Curarlas y priorizarlas para la generación de una newsletter\n",
    "\n",
    "El foco no está solo en la parte técnica, sino en justificar cada decisión\n",
    "desde un punto de vista de **valor para negocio**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486691d",
   "metadata": {},
   "source": [
    "# 1. Libraries and other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = r\"C:\\Users\\Angel\\OneDrive - Universidad Complutense de Madrid (UCM)\\Documentos\\MASTER\\99_tfm\\tfm_newsletter_ai\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from config.paths import (\n",
    "    RAW_DATA_DIR,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    DIAGNOSTICS_DIR,\n",
    "    NEWSLETTER_DIR,\n",
    "    MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821571b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m basic_preprocess\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultilingualEmbedder\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fit_kmeans, find_optimal_k\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlp'"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from config.load_config import load_config\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "\n",
    "\n",
    "# Scraping imports\n",
    "# from scraping.scraper_base import BaseScraper\n",
    "from scraping.normalization import normalize_article\n",
    "from scraping.sources.scraper_xataka import XatakaScraper\n",
    "from scraping.sources.scraper_huggingface import HuggingFaceScraper\n",
    "from scraping.sources.scraper_techcrunch import TechCrunchScraper\n",
    "from scraping.sources.scraper_aws import AWSScraper\n",
    "from scraping.sources.scraper_wired import WiredScraper\n",
    "from scraping.sources.scraper_microsoft import MicrosoftNewsScraper\n",
    "from scraping.sources.scraper_aibusiness import AIBusinessScraper\n",
    "\n",
    "# NLP imports\n",
    "from nlp.preprocessing import basic_preprocess\n",
    "from nlp.embeddings import SentenceTransformerEmbedder\n",
    "from nlp.cleaning_tfidf import clean_for_tfidf, compute_tfidf\n",
    "from nlp.clustering import fit_kmeans, find_optimal_k, compute_similarity_to_centroid\n",
    "from nlp.interpretation import top_terms_per_cluster, name_clusters\n",
    "from nlp.scoring import compute_source_score, compute_novelty_scores, compute_recency_score, compute_final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afeb1c",
   "metadata": {},
   "source": [
    "# 2. Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapers = [\n",
    "    XatakaScraper(),\n",
    "    # HuggingFaceScraper(max_pages=3),\n",
    "    TechCrunchScraper(max_pages=50),\n",
    "    AWSScraper(max_pages=50,\n",
    "               blogs=[\"machine-learning\",\n",
    "                    \"infrastructure-and-automation\",\n",
    "                    \"iot\",\n",
    "                    \"big-data\"\n",
    "                    ]\n",
    "            ),\n",
    "    WiredScraper(max_pages=50)\n",
    "    # MicrosoftNewsScraper(),\n",
    "    # AIBusinessScraper(max_pages=2)\n",
    "]\n",
    "\n",
    "articles = []\n",
    "\n",
    "for scraper in scrapers:\n",
    "    links = scraper.get_article_links()\n",
    "    for url in links:\n",
    "        article = scraper.scrape_article(url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "\n",
    "len(links)\n",
    "\n",
    "normalized_articles = [normalize_article(article) for article in articles]\n",
    "\n",
    "df = pd.DataFrame(normalized_articles)\n",
    "\n",
    "df_clean = df[df[\"is_valid\"]].copy()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "df_clean.to_csv(os.path.join(RAW_DATA_DIR, \"definite_articles.csv\"), index=False, sep=\";\")\n",
    "\n",
    "df_clean.to_parquet(os.path.join(RAW_DATA_DIR, \"definite_articles.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadb8dd",
   "metadata": {},
   "source": [
    "# 3. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68debd",
   "metadata": {},
   "source": [
    "## 3.1 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd22c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_clean.empty:\n",
    "    df_clean = pd.read_parquet(os.path.join(RAW_DATA_DIR, \"definite_articles.parquet\"))\n",
    "\n",
    "df_clean[\"text_for_embedding\"] = (\n",
    "    df_clean[\"title\"] + \". \" + df_clean[\"content\"]\n",
    ").apply(basic_preprocess)\n",
    "\n",
    "models_to_test = [\n",
    "    \"miniLM_multilingual\",\n",
    "    \"distiluse_multilingual\",\n",
    "    \"mpnet_en\"\n",
    "]\n",
    "\n",
    "embeddings_by_model = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    embedder = SentenceTransformerEmbedder(model_name)\n",
    "    embeddings = embedder.encode(df_clean[\"text_for_embedding\"].tolist())\n",
    "    embeddings_by_model[model_name] = embeddings\n",
    "    df_clean[f\"embedding_{model_name}\"] = embeddings.tolist()\n",
    "\n",
    "df_clean.to_parquet(os.path.join(PROCESSED_DATA_DIR, \"definite_articles_with_embeddings.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"miniLM_multilingual\"\n",
    "X_embeddings = np.vstack(\n",
    "    df_clean[f\"embedding_{EMBEDDING_MODEL}\"].values\n",
    ")\n",
    "\n",
    "optimal_k = find_optimal_k(X_embeddings, k_min = 4, k_max = 12)\n",
    "optimal_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be145a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = fit_kmeans(X_embeddings, n_clusters = optimal_k)\n",
    "df_clean[\"cluster\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be7001",
   "metadata": {},
   "source": [
    "## 3.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_clean.empty:\n",
    "    df_clean = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, \"definite_articles_with_embeddings.pkl\"))\n",
    "\n",
    "df[\"text_tfidf\"] = df_clean.apply(\n",
    "    lambda row: clean_for_tfidf(row[\"content\"], row[\"language\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "X_tfidf, tfidf_vectorizer = compute_tfidf(df_clean[\"text_tfidf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_terms = top_terms_per_cluster(\n",
    "    X_tfidf,\n",
    "    df_clean[\"cluster\"],\n",
    "    tfidf_vectorizer ,\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "cluster_names = name_clusters(cluster_terms)\n",
    "df_clean[\"cluster_name\"] = df_clean[\"cluster\"].map(cluster_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
