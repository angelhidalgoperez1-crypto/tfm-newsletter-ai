{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4530fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = r\"C:\\Users\\Angel\\OneDrive - Universidad Complutense de Madrid (UCM)\\Documentos\\MASTER\\99_tfm\\tfm_newsletter_ai\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94888e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping.scraper_base import BaseScraper\n",
    "import time\n",
    "\n",
    "class HuggingFaceScraper(BaseScraper):\n",
    "    def __init__(self, max_pages=50, sleep_time=1.0, scrape_community=False):\n",
    "        super().__init__(\"Hugging Face Blog\")\n",
    "        self.base_url = \"https://huggingface.co/blog\"\n",
    "        self.max_pages = max_pages\n",
    "        self.sleep_time = sleep_time\n",
    "        self.scrape_community = scrape_community\n",
    "\n",
    "    def get_article_links(self):\n",
    "        links = set()\n",
    "\n",
    "        for page in range(1, self.max_pages + 1):\n",
    "            url = self.base_url if page == 1 else f\"{self.base_url}?p={page}\"\n",
    "            soup = self.get_soup(url)\n",
    "\n",
    "            if soup is None:\n",
    "                break\n",
    "\n",
    "            # Buscar todos los enlaces a artículos\n",
    "            blog_links = soup.find_all(\"a\", href=True)\n",
    "            \n",
    "            for link in blog_links:\n",
    "                href = link[\"href\"]\n",
    "                \n",
    "                # Si scrapeamos community, incluimos todo\n",
    "                if self.scrape_community:\n",
    "                    if href.startswith(\"/blog/\") and href != \"/blog/\":\n",
    "                        full_url = f\"https://huggingface.co{href}\"\n",
    "                        links.add(full_url)\n",
    "                # Si NO scrapeamos community, excluimos esos artículos\n",
    "                else:\n",
    "                    if (href.startswith(\"/blog/\") and \n",
    "                        not href.startswith(\"/blog/community\") and \n",
    "                        href != \"/blog/\"):\n",
    "                        full_url = f\"https://huggingface.co{href}\"\n",
    "                        links.add(full_url)\n",
    "\n",
    "            time.sleep(self.sleep_time)\n",
    "\n",
    "        return list(links)\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        title = soup.find(\"h1\")\n",
    "        content_div = soup.find(\"div\", {\"class\": \"prose\"})\n",
    "\n",
    "        if not title or not content_div:\n",
    "            return None\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        return self.build_article(\n",
    "            url=url,\n",
    "            title=title.get_text(strip=True),\n",
    "            content=self.clean_text(paragraphs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78913983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artículos del blog principal: 62\n",
      "Todos los artículos: 64\n"
     ]
    }
   ],
   "source": [
    "# Opción 1: Solo artículos del blog principal (sin community)\n",
    "scraper = HuggingFaceScraper(max_pages=3, sleep_time=1, scrape_community=False)\n",
    "links = scraper.get_article_links()\n",
    "print(f\"Artículos del blog principal: {len(links)}\")\n",
    "\n",
    "# Opción 2: Todos los artículos (blog + community)\n",
    "scraper_all = HuggingFaceScraper(max_pages=3, sleep_time=1, scrape_community=True)\n",
    "links_all = scraper_all.get_article_links()\n",
    "print(f\"Todos los artículos: {len(links_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c6b427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://huggingface.co/blog/virustotal',\n",
       " 'https://huggingface.co/blog/gpt-oss-on-intel-xeon',\n",
       " 'https://huggingface.co/blog/nvidia/nvidia-isaac-for-healthcare',\n",
       " 'https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl',\n",
       " 'https://huggingface.co/blog/novita/sglang-glm4-moe',\n",
       " 'https://huggingface.co/blog/ibm-granite/granite-4-nano',\n",
       " 'https://huggingface.co/blog/nvidia/nemotron-personas-japan-ja',\n",
       " 'https://huggingface.co/blog/burtenshaw/openenv-scaling',\n",
       " 'https://huggingface.co/blog/huggingface/shifting-compute-landscape',\n",
       " 'https://huggingface.co/blog/zilliz/zilliz-semantic-highlight-model',\n",
       " 'https://huggingface.co/blog/Fannyjrd/interpreto',\n",
       " 'https://huggingface.co/blog/Arm/arm-at-pytorch-conference',\n",
       " 'https://huggingface.co/blog/amd/openroboticshackathon',\n",
       " 'https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp',\n",
       " 'https://huggingface.co/blog/aisheets-unlock-images',\n",
       " 'https://huggingface.co/blog/vibegame',\n",
       " 'https://huggingface.co/blog/rteb',\n",
       " 'https://huggingface.co/blog/ServiceNow-AI/aprielguard',\n",
       " 'https://huggingface.co/blog/microsoft/optimind',\n",
       " 'https://huggingface.co/blog/balaatdell/security-governance-performance-dell-on-premises',\n",
       " 'https://huggingface.co/blog/streaming-datasets',\n",
       " 'https://huggingface.co/blog/openenv',\n",
       " 'https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe',\n",
       " 'https://huggingface.co/blog/lerobotxnvidia-healthcare',\n",
       " 'https://huggingface.co/blog/open-responses',\n",
       " 'https://huggingface.co/blog/bigcode/arena',\n",
       " 'https://huggingface.co/blog/ngxson/make-your-own-rag',\n",
       " 'https://huggingface.co/blog/lightonai/lightonocr',\n",
       " 'https://huggingface.co/blog/waypoint-1',\n",
       " 'https://huggingface.co/blog/community?sort=trending',\n",
       " 'https://huggingface.co/blog/huggingface-hub-v1',\n",
       " 'https://huggingface.co/blog/nvidia/nemotron-personas-india',\n",
       " 'https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning',\n",
       " 'https://huggingface.co/blog/google-cloud',\n",
       " 'https://huggingface.co/blog/codelion/reverse-engineering-magic-hashhop',\n",
       " 'https://huggingface.co/blog/dots-ocr-ne',\n",
       " 'https://huggingface.co/blog/community',\n",
       " 'https://huggingface.co/blog/tokenizers',\n",
       " 'https://huggingface.co/blog/MiniMax-AI/aligning-to-what',\n",
       " 'https://huggingface.co/blog/nvidia-reachy-mini',\n",
       " 'https://huggingface.co/blog/nvidia/earth-2-open-models',\n",
       " 'https://huggingface.co/blog/tiiuae/falcon-h1-arabic',\n",
       " 'https://huggingface.co/blog/lightonai/lightonocr-2',\n",
       " 'https://huggingface.co/blog/openvino-vlm',\n",
       " 'https://huggingface.co/blog/microsoft/diff-attn-v2',\n",
       " 'https://huggingface.co/blog/lerobot-release-v040',\n",
       " 'https://huggingface.co/blog/thebajajra/rexrerankers',\n",
       " 'https://huggingface.co/blog/BenTouss/classification-showdown-oss-vs-bert',\n",
       " 'https://huggingface.co/blog/AdrianLepers/why-your-ai-strategy-needs-hugging-face-storage',\n",
       " 'https://huggingface.co/blog/ibm-research/cuga-on-hugging-face',\n",
       " 'https://huggingface.co/blog/hugging-science/ai-for-food-allergies',\n",
       " 'https://huggingface.co/blog/voice-consent-gate',\n",
       " 'https://huggingface.co/blog/florentgbelidji/vlm-ocr-recipes-gpu-infra',\n",
       " 'https://huggingface.co/blog/vansin/one-year-since-the-deepseek-moment-cn',\n",
       " 'https://huggingface.co/blog/rishiraj/challenges-of-synthetic-dataset-generation',\n",
       " 'https://huggingface.co/blog/kelseye/index-collection',\n",
       " 'https://huggingface.co/blog/nvidia/nemotron-speech-asr-scaling-voice-agents',\n",
       " 'https://huggingface.co/blog/tiiuae/emirati-benchmarks',\n",
       " 'https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face',\n",
       " 'https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment',\n",
       " 'https://huggingface.co/blog/sentence-transformers-joins-hf',\n",
       " 'https://huggingface.co/blog/ocr-open-models',\n",
       " 'https://huggingface.co/blog/intel-qwen3-agent',\n",
       " 'https://huggingface.co/blog/build-rocm-kernels']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c3d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping.scraper_base import BaseScraper\n",
    "import time\n",
    "\n",
    "class HuggingFaceScraper(BaseScraper):\n",
    "    def __init__(self, max_pages=50, sleep_time=1.0):\n",
    "        super().__init__(\"Hugging Face Blog\")\n",
    "        self.base_url = \"https://huggingface.co/blog\"\n",
    "        self.max_pages = max_pages\n",
    "        self.sleep_time = sleep_time\n",
    "\n",
    "    def get_article_links(self):\n",
    "        links = set()\n",
    "\n",
    "        for page in range(1, self.max_pages + 1):\n",
    "            url = self.base_url if page == 1 else f\"{self.base_url}?p={page}\"\n",
    "            soup = self.get_soup(url)\n",
    "\n",
    "            if soup is None:\n",
    "                break\n",
    "\n",
    "            # Buscar todos los enlaces a artículos\n",
    "            blog_links = soup.find_all(\"a\", href=True)\n",
    "            \n",
    "            for link in blog_links:\n",
    "                href = link[\"href\"]\n",
    "                \n",
    "                if href.startswith(\"/blog/\") and href != \"/blog/\":\n",
    "                    full_url = f\"https://huggingface.co{href}\"\n",
    "                    links.add(full_url)\n",
    "\n",
    "            time.sleep(self.sleep_time)\n",
    "\n",
    "        return list(links)\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        title = soup.find(\"h1\")\n",
    "        content_div = soup.find(\"div\", {\"class\": \"prose\"})\n",
    "\n",
    "        if not title or not content_div:\n",
    "            return None\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        return self.build_article(\n",
    "            url=url,\n",
    "            title=title.get_text(strip=True),\n",
    "            content=self.clean_text(paragraphs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f67a0ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog?p=93: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog?p=93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artículos del blog principal: 732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/hf-cli: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/hf-cli\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gptj-sagemaker: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gptj-sagemaker\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/constitutional_ai: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/constitutional_ai\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/python-tiny-agents: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/python-tiny-agents\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/community-tools: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/community-tools\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/stable-diffusion-xl-coreml: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/stable-diffusion-xl-coreml\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/open_rail: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/open_rail\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/using-ml-for-disasters: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/using-ml-for-disasters\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/instruction-tuning-sd: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/instruction-tuning-sd\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/segmoe: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/segmoe\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio-blocks: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio-blocks\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/collaborative-training: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/collaborative-training\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/llm-course: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/llm-course\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/bert-cpu-scaling-part-2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/bert-cpu-scaling-part-2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/aisheets-unlock-images: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/aisheets-unlock-images\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tf-serving-vision: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tf-serving-vision\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/setfit-optimum-intel: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/setfit-optimum-intel\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/arena-tts: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/arena-tts\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/controlnet: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/controlnet\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/open-r1/update-1: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/open-r1/update-1\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/hf-hub-glam-guide: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/hf-hub-glam-guide\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/argilla-chatbot: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/argilla-chatbot\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sentiment-analysis-twitter: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sentiment-analysis-twitter\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/2023-in-llms: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/2023-in-llms\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/chat-templates: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/chat-templates\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ml-web-games: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ml-web-games\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/lcm_lora: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/lcm_lora\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/codellama: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/codellama\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/kelseye/index-collection: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/kelseye/index-collection\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/duckdb-nsql-7b: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/duckdb-nsql-7b\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/timm-transformers: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/timm-transformers\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/llama-guard-4: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/llama-guard-4\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/huggingface-and-amd: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/huggingface-and-amd\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/databricks-case-study: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/databricks-case-study\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/peft: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/peft\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/getting-started-habana: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/getting-started-habana\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/kv-cache-quantization: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/kv-cache-quantization\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/zerogpu-aoti: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/zerogpu-aoti\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradient_accumulation: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradient_accumulation\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/paddlepaddle: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/paddlepaddle\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/Arm/neural-super-sampling: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/Arm/neural-super-sampling\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/pytorch-fsdp: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/pytorch-fsdp\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/train-sentence-transformers: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/train-sentence-transformers\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/pollen-vision: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/pollen-vision\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/lerobot-goes-to-driving-school: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/lerobot-goes-to-driving-school\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/community-datasets: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/community-datasets\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/transformers-model-definition: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/transformers-model-definition\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio-mcp: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio-mcp\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/vid_ds_scripts: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vid_ds_scripts\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/huggingface-amd-turin: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/huggingface-amd-turin\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/asr-diarization: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/asr-diarization\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/panel-on-hugging-face: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/panel-on-hugging-face\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/dibt: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dibt\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/os-llms: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/os-llms\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/vit-align: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vit-align\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/azure-ai-foundry: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/azure-ai-foundry\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/olympic-coder-lmstudio: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/olympic-coder-lmstudio\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ethics-soc-2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ethics-soc-2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/datasets-docs-update: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/datasets-docs-update\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tngtech/finetuning-olmocr-to-be-a-faithful-ocr-engine\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/leaderboard-upstage: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/leaderboard-upstage\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/streaming-datasets: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/streaming-datasets\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/assisted-generation-support-gaudi: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/assisted-generation-support-gaudi\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/cost-efficient-rag-applications-with-intel: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/cost-efficient-rag-applications-with-intel\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/open-deep-research: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/open-deep-research\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/setfit: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/setfit\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/synthetic-data-generator: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/synthetic-data-generator\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/Writer/announcing-palmyra-mini: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/Writer/announcing-palmyra-mini\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/textgen-pipe-gaudi: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/textgen-pipe-gaudi\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ngxson/make-your-own-rag: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ngxson/make-your-own-rag\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/text-to-webapp: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/text-to-webapp\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/graphcore-getting-started: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/graphcore-getting-started\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ml-for-games-1: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ml-for-games-1\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/mmbert: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/mmbert\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/aisheets: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/aisheets\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sd3: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sd3\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/nvidia/kvpress: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/nvidia/kvpress\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/dialog-agents: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dialog-agents\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/1b-sentence-embeddings: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/1b-sentence-embeddings\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/inference-endpoints-llm: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/inference-endpoints-llm\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fireworks-ai: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fireworks-ai\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/leaderboard-decodingtrust: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/leaderboard-decodingtrust\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/improve_parquet_dedupe: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/improve_parquet_dedupe\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/cv_state: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/cv_state\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tiiuae/e2lm-competition: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tiiuae/e2lm-competition\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/introducing-doi: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/introducing-doi\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/intel: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/intel\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/trl-ddpo: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/trl-ddpo\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/winning-aimo-progress-prize: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/winning-aimo-progress-prize\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/lightonai/lightonocr-2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/lightonai/lightonocr-2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/from-files-to-chunks: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/from-files-to-chunks\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/cosmopedia: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/cosmopedia\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/eval-on-the-hub: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/eval-on-the-hub\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/intel-sapphire-rapids-inference: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/intel-sapphire-rapids-inference\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/vllm-colocate: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vllm-colocate\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/text-to-video: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/text-to-video\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ai-residency: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ai-residency\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/putting_rl_back_in_rlhf_with_rloo\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/object-detection-leaderboard: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/object-detection-leaderboard\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/vq-diffusion: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vq-diffusion\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/dreambooth: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dreambooth\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/packing-with-FA2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/packing-with-FA2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/series-c: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/series-c\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/vansin/one-year-since-the-deepseek-moment-cn: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vansin/one-year-since-the-deepseek-moment-cn\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/deep-rl-a2c: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/deep-rl-a2c\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/deploy-tfserving-kubernetes: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/deploy-tfserving-kubernetes\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/graphcore-update: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/graphcore-update\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/infinity-cpu-performance: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/infinity-cpu-performance\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/leaderboard-japanese: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/leaderboard-japanese\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/leaderboard-llamaguard: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/leaderboard-llamaguard\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/evaluating-llm-bias: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/evaluating-llm-bias\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fastrtc-cloudflare: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fastrtc-cloudflare\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/agents: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/agents\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/cnil: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/cnil\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sdxl_lora_advanced_script: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sdxl_lora_advanced_script\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/huggingface-and-ibm: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/huggingface-and-ibm\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/3d-assets: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/3d-assets\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/codegemma: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/codegemma\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/zero-shot-vqa-docmatix: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/zero-shot-vqa-docmatix\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/big-bench-audio-release: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/big-bench-audio-release\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/lewis-tunstall-interview: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/lewis-tunstall-interview\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/amd_pervasive_developer_ai_contest: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/amd_pervasive_developer_ai_contest\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/inference-endpoints-embeddings: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/inference-endpoints-embeddings\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/diffusers-turns-1: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/diffusers-turns-1\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tf-xla-generate: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tf-xla-generate\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/dell-enterprise-hub: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dell-enterprise-hub\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/reformer: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/reformer\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/community-update: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/community-update\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/personal-copilot: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/personal-copilot\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/nvidia/nemotron-personas-india: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/nvidia/nemotron-personas-india\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fellowship: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fellowship\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio-5: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio-5\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/inferentia-llama2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/inferentia-llama2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/hugging-face-wiz-security-blog: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/hugging-face-wiz-security-blog\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio-reload: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio-reload\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/moe: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/moe\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ai-art-newsletter-jan-25: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ai-art-newsletter-jan-25\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/diffusion-models-event: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/diffusion-models-event\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tngtech/llm-performance-request-queueing: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tngtech/llm-performance-request-queueing\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/why-gradio-stands-out: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/why-gradio-stands-out\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/bamba: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/bamba\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/image-search-datasets: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/image-search-datasets\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/red-teaming: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/red-teaming\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/watermarking: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/watermarking\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/rocketmoney-case-study: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/rocketmoney-case-study\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/getting-started-with-embeddings: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/getting-started-with-embeddings\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/big-bird: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/big-bird\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gemma3n: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gemma3n\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/habana-gaudi-2-bloom: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/habana-gaudi-2-bloom\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/intro-graphml: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/intro-graphml\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fasttext: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fasttext\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/aws-marketplace: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/aws-marketplace\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sc2-instruct: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sc2-instruct\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/bridgetower: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/bridgetower\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/train-decision-transformers: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/train-decision-transformers\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/dpo-trl: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dpo-trl\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/4bit-transformers-bitsandbytes: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/chatbot-amd-gpu: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/chatbot-amd-gpu\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sentiment-analysis-python: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sentiment-analysis-python\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/novita/sglang-glm4-moe: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/novita/sglang-glm4-moe\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/owkin-substra: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/owkin-substra\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/wav2vec2-with-ngram: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/wav2vec2-with-ngram\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio-mcp-updates: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio-mcp-updates\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/pai-6-month: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/pai-6-month\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/how-to-train: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/how-to-train\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/open-llm-leaderboard-drop: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/open-llm-leaderboard-drop\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/smol2operator: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/smol2operator\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/vibegame: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/vibegame\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/safecoder-vs-closed-source-code-assistants\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sovereign-data-solution-case-study: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sovereign-data-solution-case-study\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/Hcompany/holo1: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/Hcompany/holo1\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/docmatix: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/docmatix\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/run-comfyui-workflows-on-spaces: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/run-comfyui-workflows-on-spaces\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/leaderboard-contextual: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/leaderboard-contextual\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/inference-pro: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/inference-pro\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tiny-agents: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tiny-agents\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/clipseg-zero-shot: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/clipseg-zero-shot\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fastai: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fastai\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/dots-ocr-ne: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dots-ocr-ne\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/community: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/community\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/how-to-generate: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/how-to-generate\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/autoround: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/autoround\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gaia2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gaia2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/train_memory: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/train_memory\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/elixir-bumblebee: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/elixir-bumblebee\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/filbench: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/filbench\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/writer-case-study: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/writer-case-study\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/idefics: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/idefics\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/introduction-to-ggml: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/introduction-to-ggml\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/smolvla: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/smolvla\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio-5-security: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio-5-security\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/nvidia/nemotron-speech-asr-scaling-voice-agents: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/nvidia/nemotron-speech-asr-scaling-voice-agents\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ambassadors: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ambassadors\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fine-tune-wav2vec2-english: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/audio-datasets: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/audio-datasets\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fast-mac-diffusers: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fast-mac-diffusers\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/digital-green-llm-judge: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/digital-green-llm-judge\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ml-for-games-4: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ml-for-games-4\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/diffusers-coreml: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/diffusers-coreml\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/zilliz/zilliz-semantic-highlight-model: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/zilliz/zilliz-semantic-highlight-model\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/Llama2-for-non-engineers: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/Llama2-for-non-engineers\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/deepseek-r1-aws: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/deepseek-r1-aws\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sentence-transformers-in-the-hub: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sentence-transformers-in-the-hub\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/mnist-adversarial: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/mnist-adversarial\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/jat: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/jat\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tgi-benchmarking: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tgi-benchmarking\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/microsoft/optimind: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/microsoft/optimind\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/convert-transformers-to-onnx: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/convert-transformers-to-onnx\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gemma3: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gemma3\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/audioldm2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/audioldm2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/if: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/if\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/debate: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/debate\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/synthid-text: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/synthid-text\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gemma: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gemma\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/faster-transformers: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/faster-transformers\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/gradio-joins-hf: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/gradio-joins-hf\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fine-tune-whisper: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fine-tune-whisper\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/nanovlm: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/nanovlm\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sagemaker-distributed-training-seq2seq: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sagemaker-distributed-training-seq2seq\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/iisc-huggingface-collab: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/iisc-huggingface-collab\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/npc-gigax-cubzh: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/npc-gigax-cubzh\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/pytorch-xla: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/pytorch-xla\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/leaderboard-medicalllm: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/leaderboard-medicalllm\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/data-measurements-tool: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/data-measurements-tool\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/sasha-luccioni-interview: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/sasha-luccioni-interview\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fast-diffusers-coreml: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fast-diffusers-coreml\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/hugging-science/ai-for-food-allergies: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/hugging-science/ai-for-food-allergies\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/fine-tune-xlsr-wav2vec2: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/fine-tune-xlsr-wav2vec2\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/inference-providers-groq: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/inference-providers-groq\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/habana-gaudi-2-benchmark: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/habana-gaudi-2-benchmark\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/tiiuae/emirati-benchmarks: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/tiiuae/emirati-benchmarks\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/dynamic_speculation_lookahead: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/dynamic_speculation_lookahead\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/claude-and-mcp: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/claude-and-mcp\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/ServiceNow-AI/apriel-h1: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/ServiceNow-AI/apriel-h1\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/starcoder: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/starcoder\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/autonlp-prodigy: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/autonlp-prodigy\n",
      "WARNING:root:[Hugging Face Blog] HTTP error en https://huggingface.co/blog/infini-attention: 429 Client Error: Too Many Requests for url: https://huggingface.co/blog/infini-attention\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m articles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[1;32m----> 8\u001b[0m     article \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mscrape_article(url)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m article:\n\u001b[0;32m     10\u001b[0m         articles\u001b[38;5;241m.\u001b[39mappend(article)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mHuggingFaceScraper.scrape_article\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscrape_article\u001b[39m(\u001b[38;5;28mself\u001b[39m, url):\n\u001b[1;32m---> 36\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_soup(url)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m soup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel\\OneDrive - Universidad Complutense de Madrid (UCM)\\Documentos\\MASTER\\99_tfm\\tfm_newsletter_ai\\scraping\\scraper_base.py:18\u001b[0m, in \u001b[0;36mBaseScraper.get_soup\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m         response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     19\u001b[0m         response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\http\\client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1430\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot read from timed out object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\ssl.py:1304\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1303\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Angel\\anaconda3\\Lib\\ssl.py:1138\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scraper = HuggingFaceScraper(max_pages=100, sleep_time=2)\n",
    "links = scraper.get_article_links()\n",
    "print(f\"Artículos del blog principal: {len(links)}\")\n",
    "\n",
    "articles = []\n",
    "\n",
    "for url in links:\n",
    "    article = scraper.scrape_article(url)\n",
    "    if article:\n",
    "        articles.append(article)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7095beec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error con API: 'str' object has no attribute 'get'\n",
      "Encontrados 0 posts recientes:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class HuggingFaceAPIScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://huggingface.co/api\"\n",
    "    \n",
    "    def get_blog_posts(self, limit=50, days_ago=7):\n",
    "        \"\"\"Obtiene posts del blog usando la API oficial (GRATIS)\"\"\"\n",
    "        \n",
    "        # Endpoint oficial para posts\n",
    "        url = f\"{self.base_url}/posts\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params={\"limit\": limit})\n",
    "            response.raise_for_status()\n",
    "            posts = response.json()\n",
    "            \n",
    "            # Filtrar por fecha (ej: última semana)\n",
    "            filtered_posts = []\n",
    "            cutoff_date = datetime.now() - timedelta(days=days_ago)\n",
    "            \n",
    "            for post in posts:\n",
    "                post_date = datetime.fromisoformat(post.get(\"createdAt\", \"\").replace(\"Z\", \"+00:00\"))\n",
    "                \n",
    "                if post_date >= cutoff_date:\n",
    "                    filtered_posts.append({\n",
    "                        \"title\": post.get(\"title\", \"\"),\n",
    "                        \"url\": f\"https://huggingface.co/blog/{post.get('slug', '')}\",\n",
    "                        \"author\": post.get(\"author\", {}).get(\"name\", \"\"),\n",
    "                        \"date\": post_date.strftime(\"%Y-%m-%d\"),\n",
    "                        \"summary\": post.get(\"summary\", \"\"),\n",
    "                        \"tags\": post.get(\"tags\", []),\n",
    "                        \"likes\": post.get(\"likesCount\", 0)\n",
    "                    })\n",
    "            \n",
    "            return filtered_posts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error con API: {e}\")\n",
    "            return []\n",
    "\n",
    "# USO (GRATIS):\n",
    "scraper = HuggingFaceAPIScraper()\n",
    "posts = scraper.get_blog_posts(limit=20, days_ago=30)\n",
    "\n",
    "print(f\"Encontrados {len(posts)} posts recientes:\")\n",
    "for post in posts[:5]:\n",
    "    print(f\"- {post['date']}: {post['title']} (Likes: {post['likes']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f45acfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "from scraping.sources.scraper_aws import AWSScraper\n",
    "\n",
    "aws = AWSScraper(\n",
    "    blogs=[\n",
    "        \"machine-learning\",\n",
    "        \"infrastructure-and-automation\",\n",
    "        \"iot\",\n",
    "        \"big-data\"\n",
    "    ],\n",
    "    lang=\"en\",\n",
    "    max_pages=2\n",
    ")\n",
    "\n",
    "links = aws.get_article_links()\n",
    "print(len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474ed6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "\n",
    "class AIBusinessScraper:\n",
    "    def __init__(self):\n",
    "        self.rss_url = \"https://aibusiness.com/feed/\"\n",
    "    \n",
    "    def get_articles_via_rss(self):\n",
    "        \"\"\"Usa RSS feed (legal y fácil)\"\"\"\n",
    "        feed = feedparser.parse(self.rss_url)\n",
    "        articles = []\n",
    "        \n",
    "        for entry in feed.entries[:20]:  # Últimos 20\n",
    "            articles.append({\n",
    "                \"title\": entry.title,\n",
    "                \"url\": entry.link,\n",
    "                \"date\": entry.published if 'published' in entry else \"\",\n",
    "                \"summary\": entry.summary if 'summary' in entry else \"\",\n",
    "                \"content\": entry.content[0].value if 'content' in entry else \"\"\n",
    "            })\n",
    "        \n",
    "        return articles\n",
    "\n",
    "# Uso simple y legal\n",
    "scraper = AIBusinessScraper()\n",
    "articles = scraper.get_articles_via_rss()\n",
    "\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ac0157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ddcba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping.scraper_base import BaseScraper\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class HuggingFaceScraper(BaseScraper):\n",
    "    def __init__(self, max_pages=10, sleep_time=2.0):\n",
    "        super().__init__(\"Hugging Face Blog\", base_domains=[\"huggingface.co\"])\n",
    "        self.base_url = \"https://huggingface.co/blog\"\n",
    "        self.max_pages = max_pages\n",
    "        self.sleep_time = sleep_time\n",
    "\n",
    "    def get_article_links(self):\n",
    "        links = set()\n",
    "        for page in range(1, self.max_pages + 1):\n",
    "            url = self.base_url if page == 1 else f\"{self.base_url}?p={page}\"\n",
    "            soup = self.get_soup(url)\n",
    "            if soup is None:\n",
    "                break\n",
    "\n",
    "            # Buscar enlaces a artículos (normalmente dentro de article o en h2)\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith(\"/blog/\") and href != \"/blog/\" and \"/blog/community\" not in href:\n",
    "                    full_url = \"https://huggingface.co\" + href\n",
    "                    links.add(full_url)\n",
    "\n",
    "            time.sleep(self.sleep_time)\n",
    "\n",
    "        return list(links)\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        title = soup.find(\"h1\")\n",
    "        content_div = soup.find(\"div\", class_=\"prose\")\n",
    "        if not content_div:\n",
    "            content_div = soup.find(\"div\", class_=\"markdown\")\n",
    "        if not content_div:\n",
    "            return None\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        return self.build_article(\n",
    "            url=url,\n",
    "            title=title.get_text(strip=True) if title else \"Sin título\",\n",
    "            content=self.clean_text(paragraphs)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd2fb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scraping.normalization import normalize_article\n",
    "import pandas as pd\n",
    "\n",
    "scraper = HuggingFaceScraper(max_pages=3, sleep_time=1)\n",
    "\n",
    "links = scraper.get_article_links()\n",
    "\n",
    "articles = []\n",
    "for url in links:\n",
    "    article = scraper.scrape_article(url)\n",
    "    if article:\n",
    "        articles.append(article)\n",
    "\n",
    "normalized_articles = [normalize_article(article) for article in articles]\n",
    "\n",
    "df = pd.DataFrame(normalized_articles)\n",
    "\n",
    "df_clean = df[df[\"is_valid\"]].copy()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10ad050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>scraping_date</th>\n",
       "      <th>content_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>language</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/upskill</td>\n",
       "      <td>We got Claude to teach open models how to writ...</td>\n",
       "      <td>What are agent skills? 1. Get the teacher (Cla...</td>\n",
       "      <td>2026-02-15 21:44:54.273143</td>\n",
       "      <td>8669</td>\n",
       "      <td>1442</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/nvidia/nvidia-isaa...</td>\n",
       "      <td>How to Build a Healthcare Robot from Simulatio...</td>\n",
       "      <td>Simulation has been a cornerstone in medical i...</td>\n",
       "      <td>2026-02-15 21:44:54.731683</td>\n",
       "      <td>3598</td>\n",
       "      <td>519</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/huggingface/shifti...</td>\n",
       "      <td>On the Shifting Global Compute Landscape</td>\n",
       "      <td>The status quo of AI chip usage, that was once...</td>\n",
       "      <td>2026-02-15 21:44:55.422200</td>\n",
       "      <td>20200</td>\n",
       "      <td>2942</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/flymy-ai/craft-1</td>\n",
       "      <td>CRAFT: Continuous Reasoning and Agentic Feedba...</td>\n",
       "      <td>CRAFT adds thinking into text to image generat...</td>\n",
       "      <td>2026-02-15 21:44:56.078311</td>\n",
       "      <td>1796</td>\n",
       "      <td>278</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/transformersjs-v4</td>\n",
       "      <td>Transformers.js v4 Preview: Now Available on NPM!</td>\n",
       "      <td>Performance &amp; Runtime Improvements Repository ...</td>\n",
       "      <td>2026-02-15 21:44:56.841038</td>\n",
       "      <td>6726</td>\n",
       "      <td>979</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/intel-deepmath</td>\n",
       "      <td>DeepMath: A lightweight math reasoning Agent w...</td>\n",
       "      <td>Why DeepMath? How It Works Training with GRPO...</td>\n",
       "      <td>2026-02-15 21:45:32.709601</td>\n",
       "      <td>6683</td>\n",
       "      <td>925</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/MiniMax-AI/alignin...</td>\n",
       "      <td>Aligning to What? Rethinking Agent Generalizat...</td>\n",
       "      <td>The Real Agent Alignment Problem: Benchmarks o...</td>\n",
       "      <td>2026-02-15 21:45:33.287329</td>\n",
       "      <td>3620</td>\n",
       "      <td>589</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/virustotal</td>\n",
       "      <td>Hugging Face and VirusTotal collaborate to str...</td>\n",
       "      <td>Why this matters How the collaboration works B...</td>\n",
       "      <td>2026-02-15 21:45:33.869750</td>\n",
       "      <td>1751</td>\n",
       "      <td>268</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/lerobotxnvidia-hea...</td>\n",
       "      <td>Building a Healthcare Robot from Simulation to...</td>\n",
       "      <td>A hands-on guide to collecting data, training ...</td>\n",
       "      <td>2026-02-15 21:45:34.593898</td>\n",
       "      <td>3372</td>\n",
       "      <td>486</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Hugging Face Blog</td>\n",
       "      <td>https://huggingface.co/blog/huggingface-hub-v1</td>\n",
       "      <td>huggingface_hub v1.0: Five Years of Building t...</td>\n",
       "      <td>The Story Behind the Library The Foundation Ye...</td>\n",
       "      <td>2026-02-15 21:45:35.359512</td>\n",
       "      <td>12812</td>\n",
       "      <td>1919</td>\n",
       "      <td>en</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                                url  \\\n",
       "0   Hugging Face Blog                https://huggingface.co/blog/upskill   \n",
       "1   Hugging Face Blog  https://huggingface.co/blog/nvidia/nvidia-isaa...   \n",
       "2   Hugging Face Blog  https://huggingface.co/blog/huggingface/shifti...   \n",
       "3   Hugging Face Blog       https://huggingface.co/blog/flymy-ai/craft-1   \n",
       "4   Hugging Face Blog      https://huggingface.co/blog/transformersjs-v4   \n",
       "..                ...                                                ...   \n",
       "56  Hugging Face Blog         https://huggingface.co/blog/intel-deepmath   \n",
       "57  Hugging Face Blog  https://huggingface.co/blog/MiniMax-AI/alignin...   \n",
       "58  Hugging Face Blog             https://huggingface.co/blog/virustotal   \n",
       "59  Hugging Face Blog  https://huggingface.co/blog/lerobotxnvidia-hea...   \n",
       "60  Hugging Face Blog     https://huggingface.co/blog/huggingface-hub-v1   \n",
       "\n",
       "                                                title  \\\n",
       "0   We got Claude to teach open models how to writ...   \n",
       "1   How to Build a Healthcare Robot from Simulatio...   \n",
       "2            On the Shifting Global Compute Landscape   \n",
       "3   CRAFT: Continuous Reasoning and Agentic Feedba...   \n",
       "4   Transformers.js v4 Preview: Now Available on NPM!   \n",
       "..                                                ...   \n",
       "56  DeepMath: A lightweight math reasoning Agent w...   \n",
       "57  Aligning to What? Rethinking Agent Generalizat...   \n",
       "58  Hugging Face and VirusTotal collaborate to str...   \n",
       "59  Building a Healthcare Robot from Simulation to...   \n",
       "60  huggingface_hub v1.0: Five Years of Building t...   \n",
       "\n",
       "                                              content  \\\n",
       "0   What are agent skills? 1. Get the teacher (Cla...   \n",
       "1   Simulation has been a cornerstone in medical i...   \n",
       "2   The status quo of AI chip usage, that was once...   \n",
       "3   CRAFT adds thinking into text to image generat...   \n",
       "4   Performance & Runtime Improvements Repository ...   \n",
       "..                                                ...   \n",
       "56   Why DeepMath? How It Works Training with GRPO...   \n",
       "57  The Real Agent Alignment Problem: Benchmarks o...   \n",
       "58  Why this matters How the collaboration works B...   \n",
       "59  A hands-on guide to collecting data, training ...   \n",
       "60  The Story Behind the Library The Foundation Ye...   \n",
       "\n",
       "                scraping_date  content_length  word_count language  is_valid  \n",
       "0  2026-02-15 21:44:54.273143            8669        1442       en      True  \n",
       "1  2026-02-15 21:44:54.731683            3598         519       en      True  \n",
       "2  2026-02-15 21:44:55.422200           20200        2942       en      True  \n",
       "3  2026-02-15 21:44:56.078311            1796         278       en      True  \n",
       "4  2026-02-15 21:44:56.841038            6726         979       en      True  \n",
       "..                        ...             ...         ...      ...       ...  \n",
       "56 2026-02-15 21:45:32.709601            6683         925       en      True  \n",
       "57 2026-02-15 21:45:33.287329            3620         589       en      True  \n",
       "58 2026-02-15 21:45:33.869750            1751         268       en      True  \n",
       "59 2026-02-15 21:45:34.593898            3372         486       en      True  \n",
       "60 2026-02-15 21:45:35.359512           12812        1919       en      True  \n",
       "\n",
       "[61 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27447de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import logging\n",
    "from scraping.scraper_base import BaseScraper\n",
    "\n",
    "class AIBusinessScraper(BaseScraper):\n",
    "    \"\"\"\n",
    "    Usa RSS porque el sitio web tiene Cloudflare y es difícil de scrapear directamente.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\"AI Business\", base_domains=[\"aibusiness.com\"])\n",
    "        self.feed_url = \"https://aibusiness.com/feed/\"\n",
    "\n",
    "    def get_article_links(self):\n",
    "        feed = feedparser.parse(self.feed_url)\n",
    "        links = [entry.link for entry in feed.entries]\n",
    "        return links\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        # Podríamos intentar scrapear el contenido de la página, pero con Cloudflare es complicado.\n",
    "        # Mejor devolvemos solo la URL y título del feed, y el contenido lo obtenemos del feed también.\n",
    "        # O podemos usar un método alternativo con requests-html, pero no garantizado.\n",
    "        # Aquí optamos por devolver lo que da el feed.\n",
    "        feed = feedparser.parse(self.feed_url)\n",
    "        for entry in feed.entries:\n",
    "            if entry.link == url:\n",
    "                return self.build_article(\n",
    "                    url=url,\n",
    "                    title=entry.title,\n",
    "                    content=entry.summary if hasattr(entry, 'summary') else entry.description\n",
    "                )\n",
    "        return None\n",
    "    \n",
    "scraper = AIBusinessScraper()\n",
    "\n",
    "links = scraper.get_article_links()\n",
    "len(links)\n",
    "# articles = []\n",
    "# for url in links:\n",
    "#     article = scraper.scrape_article(url)\n",
    "#     if article:\n",
    "#         articles.append(article)\n",
    "\n",
    "# normalized_articles = [normalize_article(article) for article in articles]\n",
    "\n",
    "# df = pd.DataFrame(normalized_articles)\n",
    "\n",
    "# df_clean = df[df[\"is_valid\"]].copy()\n",
    "# df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e534d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scraping.scraper_base import BaseScraper\n",
    "import time\n",
    "\n",
    "class AllenAIScraper(BaseScraper):\n",
    "    def __init__(self, max_pages=5, sleep_time=1.0):\n",
    "        super().__init__(\"Allen AI\", base_domains=[\"allenai.org\"])\n",
    "        self.base_url = \"https://allenai.org/research\"\n",
    "        self.max_pages = max_pages\n",
    "        self.sleep_time = sleep_time\n",
    "\n",
    "    def get_article_links(self):\n",
    "        links = set()\n",
    "        # El blog de Allen AI parece tener paginación por ?page=\n",
    "        for page in range(1, self.max_pages + 1):\n",
    "            url = f\"{self.base_url}?page={page}\" if page > 1 else self.base_url\n",
    "            soup = self.get_soup(url)\n",
    "            if soup is None:\n",
    "                break\n",
    "\n",
    "            # Buscar enlaces a artículos (probablemente dentro de article o h2)\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a['href']\n",
    "                if href.startswith(\"/research/\") and href != \"/research/\":\n",
    "                    full_url = \"https://allenai.org\" + href\n",
    "                    links.add(full_url)\n",
    "                elif href.startswith(\"https://allenai.org/research/\"):\n",
    "                    links.add(href)\n",
    "\n",
    "            time.sleep(self.sleep_time)\n",
    "\n",
    "        return list(links)\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        title = soup.find(\"h1\")\n",
    "        # El contenido puede estar en un div con clase \"blog-post\" o \"content\"\n",
    "        content_div = soup.find(\"div\", class_=\"blog-post\") or soup.find(\"div\", class_=\"content\") or soup.find(\"article\")\n",
    "        if not content_div:\n",
    "            return None\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        return self.build_article(\n",
    "            url=url,\n",
    "            title=title.get_text(strip=True) if title else \"Sin título\",\n",
    "            content=self.clean_text(paragraphs)\n",
    "        )\n",
    "    \n",
    "\n",
    "scraper = AllenAIScraper(max_pages=6, sleep_time=1)\n",
    "links = scraper.get_article_links()\n",
    "len(links)\n",
    "# articles = []\n",
    "# for url in links:\n",
    "#     article = scraper.scrape_article(url)\n",
    "#     if article:\n",
    "#         articles.append(article)\n",
    "\n",
    "# normalized_articles = [normalize_article(article) for article in articles]\n",
    "\n",
    "# df = pd.DataFrame(normalized_articles)\n",
    "\n",
    "# df_clean = df[df[\"is_valid\"]].copy()\n",
    "# df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b99388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scraping.scraper_base import BaseScraper\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "\n",
    "class OpenAIScraper(BaseScraper):\n",
    "    def __init__(self, max_articles=50, sleep_time=1.0):\n",
    "        super().__init__(\"OpenAI\", base_domains=[\"openai.com\"])\n",
    "        self.sitemap_url = \"https://openai.com/sitemap.xml\"\n",
    "        self.max_articles = max_articles\n",
    "        self.sleep_time = sleep_time\n",
    "\n",
    "    def get_article_links(self):\n",
    "        # Obtener todas las URLs del sitemap que correspondan a blog\n",
    "        try:\n",
    "            response = requests.get(self.sitemap_url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            root = ET.fromstring(response.content)\n",
    "            # Espacio de nombres típico de sitemap\n",
    "            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            urls = []\n",
    "            for url in root.findall('sm:url', ns):\n",
    "                loc = url.find('sm:loc', ns).text\n",
    "                if '/blog/' in loc:\n",
    "                    urls.append(loc)\n",
    "            return urls[:self.max_articles]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"[OpenAI] Error obteniendo sitemap: {e}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        title = soup.find(\"h1\")\n",
    "        # El contenido puede estar en un artículo o div con clase específica\n",
    "        content_div = soup.find(\"article\") or soup.find(\"div\", class_=\"prose\") or soup.find(\"div\", class_=\"content\")\n",
    "        if not content_div:\n",
    "            return None\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        return self.build_article(\n",
    "            url=url,\n",
    "            title=title.get_text(strip=True) if title else \"Sin título\",\n",
    "            content=self.clean_text(paragraphs)\n",
    "        )\n",
    "    \n",
    "scraper = OpenAIScraper(max_articles=30, sleep_time=1)\n",
    "links = scraper.get_article_links()\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4612e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping.scraper_base import BaseScraper\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class MicrosoftAIScraper(BaseScraper):\n",
    "    def __init__(self, sources=None, max_pages=5, sleep_time=2.0):\n",
    "        super().__init__(\"Microsoft AI\", base_domains=[\"microsoft.com\", \"azure.microsoft.com\"])\n",
    "        self.sources = sources or [\n",
    "            {\"name\": \"news\", \"url\": \"https://news.microsoft.com/source/topics/ai/\", \"pagination\": \"page/{page}/\"},\n",
    "            # {\"name\": \"azure\", \"url\": \"https://azure.microsoft.com/en-us/blog/\", \"pagination\": \"page/{page}/\"},\n",
    "            {\"name\": \"ai blog\", \"url\": \"https://blogs.microsoft.com/ai/\", \"pagination\": \"page/{page}/\"}\n",
    "        ]\n",
    "        self.max_pages = max_pages\n",
    "        self.sleep_time = sleep_time\n",
    "\n",
    "    def get_article_links(self):\n",
    "        links = set()\n",
    "        for source in self.sources:\n",
    "            base = source[\"url\"]\n",
    "            for page in range(1, self.max_pages + 1):\n",
    "                if page == 1:\n",
    "                    url = base\n",
    "                else:\n",
    "                    url = base + source[\"pagination\"].format(page=page)\n",
    "                soup = self.get_soup(url)\n",
    "                if soup is None:\n",
    "                    break\n",
    "\n",
    "                # Buscar enlaces a artículos\n",
    "                for a in soup.find_all(\"a\", href=True):\n",
    "                    href = a['href']\n",
    "                    # Filtrar enlaces que parezcan artículos (que contengan fecha o /año/mes/)\n",
    "                    if any(pat in href for pat in ['/202', '/20', '/article', '/post']):\n",
    "                        if href.startswith('/'):\n",
    "                            full_url = \"https://\" + source[\"url\"].split('/')[2] + href\n",
    "                        elif href.startswith('http'):\n",
    "                            full_url = href\n",
    "                        else:\n",
    "                            full_url = base.rstrip('/') + '/' + href.lstrip('/')\n",
    "                        if any(domain in full_url for domain in self.base_domains):\n",
    "                            links.add(full_url)\n",
    "\n",
    "                time.sleep(self.sleep_time)\n",
    "\n",
    "        return list(links)\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        title = soup.find(\"h1\")\n",
    "        # Contenido puede variar según el blog\n",
    "        content_div = (soup.find(\"article\") or \n",
    "                       soup.find(\"div\", class_=\"entry-content\") or \n",
    "                       soup.find(\"div\", class_=\"post-content\") or\n",
    "                       soup.find(\"div\", class_=\"content\"))\n",
    "        if not content_div:\n",
    "            return None\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        return self.build_article(\n",
    "            url=url,\n",
    "            title=title.get_text(strip=True) if title else \"Sin título\",\n",
    "            content=self.clean_text(paragraphs)\n",
    "        )\n",
    "    \n",
    "# scraper = MicrosoftAIScraper(max_pages=2, sleep_time=1)\n",
    "# links = scraper.get_article_links()\n",
    "# len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3443872b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Microsoft AI] HTTP error en https://news.microsoft.com/source/topics/ai/page/12/: 404 Client Error: Not Found for url: https://news.microsoft.com/source/topics/ai/page/12/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper = MicrosoftAIScraper(max_pages=30, sleep_time=1)\n",
    "links = scraper.get_article_links()\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336a90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNewsScraper(BaseScraper):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Google News\")\n",
    "        self.base_url = \"https://news.google.com\"\n",
    "    \n",
    "    def get_ai_news(self):\n",
    "        # Google News permite scraping fácil\n",
    "        soup = self.get_soup(f\"{self.base_url}/search?q=AI+artificial+intelligence&hl=es\")\n",
    "        links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            if \"article\" in a[\"href\"]:\n",
    "                links.append(self.base_url + a[\"href\"][1:])  # Remover el . inicial\n",
    "        return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67929bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = GoogleNewsScraper()\n",
    "links = scraper.get_ai_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e5fb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a44504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scrapeando página 1: https://openai.com/blog\n",
      "WARNING:root:[OpenAI Blog] HTTP error en https://openai.com/blog: 403 Client Error: Forbidden for url: https://openai.com/blog\n",
      "WARNING:root:No se pudo obtener https://openai.com/blog\n",
      "INFO:root:Total enlaces encontrados: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artículos encontrados: 0\n"
     ]
    }
   ],
   "source": [
    "from scraping.scraper_base import BaseScraper\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class OpenAIScraper(BaseScraper):\n",
    "    \"\"\"\n",
    "    Scraper para el blog de OpenAI (https://openai.com/blog)\n",
    "    Funciona con BeautifulSoup sin necesidad de JavaScript.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_pages=5, sleep_time=2.0):\n",
    "        super().__init__(\"OpenAI Blog\", base_domains=[\"openai.com\"])\n",
    "        self.base_url = \"https://openai.com/blog\"\n",
    "        self.max_pages = max_pages\n",
    "        self.sleep_time = sleep_time\n",
    "\n",
    "        # Headers realistas para evitar bloqueos\n",
    "        self.headers.update({\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"es-ES,es;q=0.9,en;q=0.8\",\n",
    "        })\n",
    "\n",
    "    def get_article_links(self):\n",
    "        \"\"\"\n",
    "        Obtiene los enlaces a los artículos del blog de OpenAI.\n",
    "        OpenAI usa paginación por scroll infinito, pero la página inicial ya carga ~20 artículos.\n",
    "        \"\"\"\n",
    "        links = set()\n",
    "\n",
    "        # OpenAI no tiene paginación tradicional (?page=2), pero la URL base carga todos los recientes.\n",
    "        # Para obtener más, podemos usar el parámetro ?page (aunque no es oficial, a veces funciona)\n",
    "        for page in range(1, self.max_pages + 1):\n",
    "            if page == 1:\n",
    "                url = self.base_url\n",
    "            else:\n",
    "                # Parámetro de paginación no oficial, pero a veces devuelve artículos antiguos\n",
    "                url = f\"{self.base_url}?page={page}\"\n",
    "\n",
    "            logging.info(f\"Scrapeando página {page}: {url}\")\n",
    "            soup = self.get_soup(url)\n",
    "\n",
    "            if soup is None:\n",
    "                logging.warning(f\"No se pudo obtener {url}\")\n",
    "                continue\n",
    "\n",
    "            # Buscar enlaces a artículos individuales\n",
    "            # Estructura típica: <a href=\"/blog/titulo-del-articulo\"> dentro de un contenedor\n",
    "            article_links = soup.select('a[href^=\"/blog/\"]')\n",
    "\n",
    "            # Filtrar enlaces que no sean artículos (como /blog, /blog?page=...)\n",
    "            for a in article_links:\n",
    "                href = a.get(\"href\")\n",
    "                if href and href.startswith(\"/blog/\") and href != \"/blog\":\n",
    "                    # Excluir enlaces que contengan parámetros\n",
    "                    if \"?\" not in href and \"#\" not in href:\n",
    "                        full_url = f\"https://openai.com{href}\"\n",
    "                        links.add(full_url)\n",
    "\n",
    "            # Si no se encuentran más enlaces, salir\n",
    "            if len(article_links) == 0:\n",
    "                break\n",
    "\n",
    "            time.sleep(self.sleep_time)\n",
    "\n",
    "        logging.info(f\"Total enlaces encontrados: {len(links)}\")\n",
    "        return list(links)\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        \"\"\"\n",
    "        Extrae el título y contenido de un artículo de OpenAI.\n",
    "        \"\"\"\n",
    "        soup = self.get_soup(url)\n",
    "        if soup is None:\n",
    "            return None\n",
    "\n",
    "        # Título: suele estar en un h1\n",
    "        title_elem = soup.find(\"h1\")\n",
    "        if not title_elem:\n",
    "            # Alternativa: meta og:title\n",
    "            meta_title = soup.find(\"meta\", property=\"og:title\")\n",
    "            title = meta_title[\"content\"] if meta_title else \"Sin título\"\n",
    "        else:\n",
    "            title = title_elem.get_text(strip=True)\n",
    "\n",
    "        # Contenido: OpenAI usa un div con clase \"prose\" o \"article-content\"\n",
    "        content_div = soup.select_one(\"div.prose, div.article-content, article\")\n",
    "        if not content_div:\n",
    "            logging.warning(f\"No se encontró contenido en {url}\")\n",
    "            return None\n",
    "\n",
    "        # Extraer párrafos\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        # Limpiar y filtrar párrafos muy cortos o irrelevantes\n",
    "        valid_paragraphs = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if len(text) > 30 and not text.startswith(\"Share\"):\n",
    "                valid_paragraphs.append(p)\n",
    "\n",
    "        if not valid_paragraphs:\n",
    "            return None\n",
    "\n",
    "        content = self.clean_text(valid_paragraphs)\n",
    "\n",
    "        return self.build_article(\n",
    "            url=url,\n",
    "            title=title,\n",
    "            content=content\n",
    "        )\n",
    "\n",
    "\n",
    "# Ejemplo de uso rápido (para probar)\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = OpenAIScraper(max_pages=1, sleep_time=3)\n",
    "    links = scraper.get_article_links()\n",
    "    print(f\"Artículos encontrados: {len(links)}\")\n",
    "\n",
    "    if links:\n",
    "        # Probar el primer artículo\n",
    "        articulo = scraper.scrape_article(links[0])\n",
    "        if articulo:\n",
    "            print(f\"Título: {articulo['title']}\")\n",
    "            print(f\"Contenido: {articulo['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c737ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scrapeando página 1: https://openai.com/blog\n",
      "WARNING:root:[OpenAI Blog] HTTP error en https://openai.com/blog: 403 Client Error: Forbidden for url: https://openai.com/blog\n",
      "WARNING:root:No se pudo obtener https://openai.com/blog\n",
      "INFO:root:Total enlaces encontrados: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtenidos 0 artículos\n"
     ]
    }
   ],
   "source": [
    "# En tu script principal\n",
    "scraper = OpenAIScraper(max_pages=1, sleep_time=3)\n",
    "links = scraper.get_article_links()[:5]  # Solo 5 para prueba\n",
    "\n",
    "articulos = []\n",
    "for url in links:\n",
    "    articulo = scraper.scrape_article(url)\n",
    "    if articulo:\n",
    "        articulos.append(articulo)\n",
    "    time.sleep(2)  # Delay entre artículos\n",
    "\n",
    "print(f\"Obtenidos {len(articulos)} artículos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb124cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class AINewsletterCollector:\n",
    "    \"\"\"Recolector de noticias para newsletter de IA\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sources = [\n",
    "            {\"name\": \"OpenAI\", \"url\": \"https://openai.com/news.xml\"},\n",
    "            {\"name\": \"DeepMind\", \"url\": \"https://deepmind.google/blog/rss.xml\"},\n",
    "            {\"name\": \"Anthropic\", \"url\": \"https://www.anthropic.com/rss.xml\"},\n",
    "            {\"name\": \"Hugging Face\", \"url\": \"https://huggingface.co/blog/feed.xml\"},\n",
    "            {\"name\": \"TechCrunch AI\", \"url\": \"https://techcrunch.com/tag/artificial-intelligence/feed/\"},\n",
    "            {\"name\": \"MIT AI\", \"url\": \"http://news.mit.edu/topic/mitartificial-intelligence2-rss.xml\"},\n",
    "        ]\n",
    "    \n",
    "    def get_news_last_days(self, days=7):\n",
    "        \"\"\"Obtiene noticias de los últimos X días\"\"\"\n",
    "        all_news = []\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        \n",
    "        for source in self.sources:\n",
    "            print(f\"Procesando {source['name']}...\")\n",
    "            feed = feedparser.parse(source[\"url\"])\n",
    "            \n",
    "            for entry in feed.entries:\n",
    "                # Intentar obtener fecha\n",
    "                pub_str = entry.get('published', entry.get('updated', ''))\n",
    "                try:\n",
    "                    pub_date = datetime.strptime(pub_str, '%a, %d %b %Y %H:%M:%S %z')\n",
    "                    pub_date = pub_date.replace(tzinfo=None)  # Quitar timezone\n",
    "                except:\n",
    "                    # Si no podemos parsear, asumir que es reciente\n",
    "                    pub_date = datetime.now()\n",
    "                \n",
    "                if pub_date >= cutoff:\n",
    "                    all_news.append({\n",
    "                        'source': source['name'],\n",
    "                        'title': entry.get('title', ''),\n",
    "                        'url': entry.get('link', ''),\n",
    "                        'date': pub_date.strftime('%Y-%m-%d'),\n",
    "                        'summary': entry.get('summary', '')[:300],\n",
    "                    })\n",
    "        \n",
    "        # Ordenar por fecha\n",
    "        all_news.sort(key=lambda x: x['date'], reverse=True)\n",
    "        return all_news\n",
    "    \n",
    "    def generate_newsletter_html(self, news):\n",
    "        \"\"\"Genera HTML para newsletter\"\"\"\n",
    "        html = \"<h1>📰 Newsletter IA Semanal</h1>\\n\"\n",
    "        \n",
    "        # Agrupar por fuente\n",
    "        by_source = {}\n",
    "        for item in news:\n",
    "            by_source.setdefault(item['source'], []).append(item)\n",
    "        \n",
    "        for source, items in by_source.items():\n",
    "            html += f\"<h2>{source}</h2>\\n<ul>\\n\"\n",
    "            for item in items:\n",
    "                html += f'  <li><a href=\"{item[\"url\"]}\">{item[\"title\"]}</a> - {item[\"date\"]}<br>'\n",
    "                html += f'  <small>{item[\"summary\"]}</small></li>\\n'\n",
    "            html += \"</ul>\\n\"\n",
    "        \n",
    "        return html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a46e3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando OpenAI...\n",
      "Procesando DeepMind...\n",
      "Procesando Anthropic...\n",
      "Procesando Hugging Face...\n",
      "Procesando TechCrunch AI...\n",
      "Procesando MIT AI...\n",
      "Noticias de los últimos 7 días: 741\n"
     ]
    }
   ],
   "source": [
    "# Uso\n",
    "collector = AINewsletterCollector()\n",
    "news = collector.get_news_last_days(days=7)\n",
    "print(f\"Noticias de los últimos 7 días: {len(news)}\")\n",
    "\n",
    "# Generar HTML\n",
    "html = collector.generate_newsletter_html(news)\n",
    "with open(\"newsletter_ia_semanal.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33966d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h1>📰 Newsletter IA Semanal</h1>\\n<h2>Hugging Face</h2>\\n<ul>\\n  <li><a href=\"https://huggingface.co/blog/custom-cuda-kernels-agent-skills\">Custom Kernels for All from Codex and Claude</a> - 2026-02-16<br>  <small></small></li>\\n  <li><a href=\"https://huggingface.co/blog/openenv-turing\">OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments</a> - 2026-02-16<br>  <small></small></li>\\n  <li><a href=\"https://huggingface.co/blog/transformersjs-v4\">Transformers.js v4 Preview: Now Av'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d92931a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Obteniendo RSS de OpenAI...\n",
      "INFO:root:Obteniendo RSS de DeepMind...\n",
      "INFO:root:Obteniendo RSS de Anthropic...\n",
      "INFO:root:Obteniendo RSS de Hugging Face...\n",
      "INFO:root:Obteniendo RSS de TechCrunch AI...\n",
      "INFO:root:Obteniendo RSS de MIT AI...\n",
      "INFO:root:Total artículos obtenidos del RSS: 741\n",
      "INFO:root:Procesando 1/5: Custom Kernels for All from Codex and Claude...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artículos encontrados en RSS: 741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:  Scraping exitoso\n",
      "INFO:root:Procesando 2/5: OpenEnv in Practice: Evaluating Tool-Using Agents in Real-Wo...\n",
      "INFO:root:  Scraping exitoso\n",
      "INFO:root:Procesando 3/5: Transformers.js v4 Preview: Now Available on NPM!...\n",
      "INFO:root:  Scraping exitoso\n",
      "INFO:root:Procesando 4/5: Introducing SyGra Studio...\n",
      "INFO:root:  Scraping exitoso\n",
      "INFO:root:Procesando 5/5: Nemotron ColEmbed V2: Raising the Bar for Multimodal Retriev...\n",
      "INFO:root:  Scraping exitoso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsletter generada: newsletter_completa.html\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "from scraping.scraper_base import BaseScraper  # tu clase base\n",
    "\n",
    "# Configuración básica de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class RSSContentScraper(BaseScraper):\n",
    "    \"\"\"\n",
    "    Extiende BaseScraper para añadir extracción de contenido específica por dominio.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_name, base_domains):\n",
    "        super().__init__(source_name, base_domains)\n",
    "        # Headers más realistas (opcional, ya los tiene BaseScraper)\n",
    "        self.headers.update({\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Accept-Language\": \"es-ES,es;q=0.9,en;q=0.8\",\n",
    "        })\n",
    "\n",
    "    def extract_full_content(self, url):\n",
    "        \"\"\"\n",
    "        Intenta extraer el contenido completo de un artículo según el dominio.\n",
    "        Retorna el texto del contenido o None si falla.\n",
    "        \"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc.replace(\"www.\", \"\")\n",
    "\n",
    "        soup = self.get_soup(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        # Diccionario de selectores por dominio (puedes ampliarlo)\n",
    "        selectors = {\n",
    "            \"openai.com\": \"div.prose, article\",\n",
    "            \"deepmind.google\": \"div[class*='blog-post'], article\",\n",
    "            \"anthropic.com\": \"div.content, article\",\n",
    "            \"huggingface.co\": \"div.prose\",\n",
    "            \"techcrunch.com\": \"div.article-content, div.post-content\",\n",
    "            \"news.mit.edu\": \"div.entry-content\",\n",
    "            \"ai.meta.com\": \"div.blog-content\",\n",
    "            \"wired.com\": \"div.article__body\",\n",
    "            \"aibusiness.com\": \"div.article-content\",  # probar, quizás no funcione\n",
    "        }\n",
    "\n",
    "        selector = selectors.get(domain, \"article, main, .content, .post-content\")  # fallback genérico\n",
    "        content_div = soup.select_one(selector)\n",
    "\n",
    "        if not content_div:\n",
    "            # Intento más genérico: buscar el primer div con muchos párrafos\n",
    "            candidates = soup.find_all('div')\n",
    "            for div in candidates:\n",
    "                p_count = len(div.find_all('p'))\n",
    "                if p_count > 3:\n",
    "                    content_div = div\n",
    "                    break\n",
    "\n",
    "        if not content_div:\n",
    "            logging.warning(f\"No se pudo encontrar contenido para {url}\")\n",
    "            return None\n",
    "\n",
    "        paragraphs = content_div.find_all('p')\n",
    "        if not paragraphs:\n",
    "            return None\n",
    "\n",
    "        # Limpiar párrafos muy cortos o sospechosos (publicidad, etc.)\n",
    "        clean_paras = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if len(text) > 40 and not any(x in text.lower() for x in ['copyright', 'all rights reserved', 'compartir en']):\n",
    "                clean_paras.append(p)\n",
    "\n",
    "        if not clean_paras:\n",
    "            return None\n",
    "\n",
    "        return self.clean_text(clean_paras)\n",
    "\n",
    "\n",
    "class AINewsletterCollector:\n",
    "    \"\"\"\n",
    "    Recolecta noticias de múltiples fuentes vía RSS y luego enriquece con contenido completo.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, days=7):\n",
    "        self.days = days\n",
    "        self.sources = [\n",
    "            {\"name\": \"OpenAI\", \"url\": \"https://openai.com/news.xml\", \"domain\": \"openai.com\"},\n",
    "            {\"name\": \"DeepMind\", \"url\": \"https://deepmind.google/blog/rss.xml\", \"domain\": \"deepmind.google\"},\n",
    "            {\"name\": \"Anthropic\", \"url\": \"https://www.anthropic.com/rss.xml\", \"domain\": \"anthropic.com\"},\n",
    "            {\"name\": \"Hugging Face\", \"url\": \"https://huggingface.co/blog/feed.xml\", \"domain\": \"huggingface.co\"},\n",
    "            {\"name\": \"TechCrunch AI\", \"url\": \"https://techcrunch.com/tag/artificial-intelligence/feed/\", \"domain\": \"techcrunch.com\"},\n",
    "            {\"name\": \"MIT AI\", \"url\": \"http://news.mit.edu/topic/mitartificial-intelligence2-rss.xml\", \"domain\": \"news.mit.edu\"},\n",
    "        ]\n",
    "        # Inicializar scrapers por dominio (para reutilizar)\n",
    "        self.scrapers = {}\n",
    "        for source in self.sources:\n",
    "            domain = source[\"domain\"]\n",
    "            if domain not in self.scrapers:\n",
    "                self.scrapers[domain] = RSSContentScraper(source[\"name\"], [domain])\n",
    "\n",
    "    def fetch_articles_from_rss(self):\n",
    "        \"\"\"Obtiene artículos básicos (sin contenido completo) de los últimos días.\"\"\"\n",
    "        articles = []\n",
    "        cutoff = datetime.now() - timedelta(days=self.days)\n",
    "\n",
    "        for source in self.sources:\n",
    "            logging.info(f\"Obteniendo RSS de {source['name']}...\")\n",
    "            feed = feedparser.parse(source[\"url\"])\n",
    "\n",
    "            for entry in feed.entries:\n",
    "                # Parsear fecha\n",
    "                pub_str = entry.get('published', entry.get('updated', ''))\n",
    "                try:\n",
    "                    pub_date = datetime.strptime(pub_str, '%a, %d %b %Y %H:%M:%S %z')\n",
    "                    pub_date = pub_date.replace(tzinfo=None)\n",
    "                except:\n",
    "                    pub_date = datetime.now()  # si no hay fecha, asumimos reciente\n",
    "\n",
    "                if pub_date >= cutoff:\n",
    "                    # Intentar obtener contenido del RSS (a veces viene completo)\n",
    "                    rss_content = ''\n",
    "                    if 'content' in entry and entry.content:\n",
    "                        rss_content = entry.content[0].value\n",
    "                    elif 'summary' in entry:\n",
    "                        rss_content = entry.summary\n",
    "\n",
    "                    articles.append({\n",
    "                        'source': source['name'],\n",
    "                        'title': entry.get('title', ''),\n",
    "                        'url': entry.get('link', ''),\n",
    "                        'date': pub_date.strftime('%Y-%m-%d'),\n",
    "                        'rss_content': rss_content,  # guardamos lo que trae el RSS\n",
    "                        'full_content': None,        # lo rellenaremos después\n",
    "                    })\n",
    "            time.sleep(1)  # pequeño delay entre fuentes\n",
    "\n",
    "        # Ordenar por fecha\n",
    "        articles.sort(key=lambda x: x['date'], reverse=True)\n",
    "        logging.info(f\"Total artículos obtenidos del RSS: {len(articles)}\")\n",
    "        return articles\n",
    "\n",
    "    def enrich_with_full_content(self, articles, max_articles=10, delay_between=3):\n",
    "        \"\"\"\n",
    "        Para cada artículo, intenta obtener el contenido completo:\n",
    "        - Si el RSS ya trae contenido sustancial (>500 chars), lo usa.\n",
    "        - Si no, hace scraping con el scraper correspondiente.\n",
    "        Solo procesa los primeros `max_articles` para no saturar.\n",
    "        \"\"\"\n",
    "        enriched = []\n",
    "        for i, art in enumerate(articles[:max_articles]):\n",
    "            logging.info(f\"Procesando {i+1}/{min(max_articles, len(articles))}: {art['title'][:60]}...\")\n",
    "\n",
    "            # 1. Comprobar si el RSS ya trae contenido suficiente\n",
    "            if art['rss_content'] and len(art['rss_content']) > 500:\n",
    "                art['full_content'] = art['rss_content']\n",
    "                logging.info(\"  Usando contenido del RSS\")\n",
    "            else:\n",
    "                # 2. Scraping\n",
    "                domain = art['url'].split('/')[2].replace('www.', '')\n",
    "                scraper = self.scrapers.get(domain)\n",
    "                if not scraper:\n",
    "                    # Buscar por dominio en sources (por si hay varios dominios por fuente)\n",
    "                    for source in self.sources:\n",
    "                        if source['name'] == art['source']:\n",
    "                            domain = source['domain']\n",
    "                            scraper = self.scrapers.get(domain)\n",
    "                            break\n",
    "                if scraper:\n",
    "                    content = scraper.extract_full_content(art['url'])\n",
    "                    if content:\n",
    "                        art['full_content'] = content\n",
    "                        logging.info(\"  Scraping exitoso\")\n",
    "                    else:\n",
    "                        art['full_content'] = art['rss_content'] or \"Contenido no disponible\"\n",
    "                        logging.warning(\"  Scraping falló, se usa contenido parcial\")\n",
    "                else:\n",
    "                    art['full_content'] = art['rss_content'] or \"Contenido no disponible\"\n",
    "                    logging.warning(f\"  No hay scraper para dominio {domain}\")\n",
    "\n",
    "            enriched.append(art)\n",
    "            if i < max_articles - 1:\n",
    "                time.sleep(delay_between)  # delay entre artículos\n",
    "\n",
    "        return enriched\n",
    "\n",
    "    def generate_newsletter_html(self, articles):\n",
    "        \"\"\"Genera HTML con título, fecha, enlace y contenido completo.\"\"\"\n",
    "        html = \"<h1>📰 Newsletter IA Semanal</h1>\\n\"\n",
    "        for art in articles:\n",
    "            html += f\"<h2><a href='{art['url']}'>{art['title']}</a></h2>\\n\"\n",
    "            html += f\"<p><strong>{art['source']}</strong> - {art['date']}</p>\\n\"\n",
    "            html += f\"<div>{art['full_content']}</div>\\n\"\n",
    "            html += \"<hr>\\n\"\n",
    "        return html\n",
    "\n",
    "\n",
    "# ========== EJEMPLO DE USO ==========\n",
    "if __name__ == \"__main__\":\n",
    "    collector = AINewsletterCollector(days=7)\n",
    "\n",
    "    # Paso 1: obtener artículos del RSS\n",
    "    articulos_rss = collector.fetch_articles_from_rss()\n",
    "    print(f\"Artículos encontrados en RSS: {len(articulos_rss)}\")\n",
    "\n",
    "    # Paso 2: enriquecer con contenido completo (solo los 5 primeros para pruebas)\n",
    "    articulos_completos = collector.enrich_with_full_content(articulos_rss, max_articles=5, delay_between=5)\n",
    "\n",
    "    # Paso 3: generar HTML final\n",
    "    html_final = collector.generate_newsletter_html(articulos_completos)\n",
    "\n",
    "    with open(\"newsletter_completa.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_final)\n",
    "\n",
    "    print(\"Newsletter generada: newsletter_completa.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ee776a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30794"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1beabe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import time\n",
    "import logging\n",
    "from scraping.scraper_base import BaseScraper\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class RSSContentCollector2:\n",
    "    \"\"\"\n",
    "    Colector que combina RSS + scraping para obtener contenido completo.\n",
    "    Devuelve artículos en el mismo formato que tus scrapers directos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Fuentes: nombre y feed RSS\n",
    "        self.sources = [\n",
    "            {\"name\": \"OpenAI\", \"rss\": \"https://openai.com/news.xml\"},\n",
    "            {\"name\": \"DeepMind\", \"rss\": \"https://deepmind.google/blog/rss.xml\"},\n",
    "            {\"name\": \"Anthropic\", \"rss\": \"https://www.anthropic.com/rss.xml\"},\n",
    "            # {\"name\": \"Hugging Face\", \"rss\": \"https://huggingface.co/blog/feed.xml\"},\n",
    "            # {\"name\": \"TechCrunch AI\", \"rss\": \"https://techcrunch.com/tag/artificial-intelligence/feed/\"},\n",
    "            {\"name\": \"MIT AI\", \"rss\": \"http://news.mit.edu/topic/mitartificial-intelligence2-rss.xml\"},\n",
    "        ]\n",
    "        # Scrapers por dominio (reutilizables)\n",
    "        self.scrapers = {}\n",
    "\n",
    "    def _get_scraper(self, source_name, url):\n",
    "        \"\"\"Obtiene o crea un scraper para el dominio de la URL.\"\"\"\n",
    "        from urllib.parse import urlparse\n",
    "        domain = urlparse(url).netloc.replace(\"www.\", \"\")\n",
    "        \n",
    "        if domain not in self.scrapers:\n",
    "            # Crear un scraper específico para este dominio\n",
    "            class DynamicScraper(BaseScraper):\n",
    "                def __init__(self, name, domain):\n",
    "                    super().__init__(name, base_domains=[domain])\n",
    "                    self.headers.update({\n",
    "                        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "                    })\n",
    "                \n",
    "                def scrape_article(self, url):\n",
    "                    soup = self.get_soup(url)\n",
    "                    if not soup:\n",
    "                        return None\n",
    "                    \n",
    "                    # Título\n",
    "                    title = soup.find(\"h1\")\n",
    "                    if not title:\n",
    "                        title = soup.find(\"meta\", property=\"og:title\")\n",
    "                        title = title[\"content\"] if title else \"Sin título\"\n",
    "                    else:\n",
    "                        title = title.get_text(strip=True)\n",
    "                    \n",
    "                    # Contenido - buscar contenedor principal\n",
    "                    content_div = (\n",
    "                        soup.find(\"div\", class_=\"prose\") or\n",
    "                        soup.find(\"article\") or\n",
    "                        soup.find(\"main\") or\n",
    "                        soup.find(\"div\", class_=\"content\")\n",
    "                    )\n",
    "                    \n",
    "                    if not content_div:\n",
    "                        return None\n",
    "                    \n",
    "                    paragraphs = content_div.find_all(\"p\")\n",
    "                    if not paragraphs:\n",
    "                        return None\n",
    "                    \n",
    "                    # Filtrar párrafos cortos o basura\n",
    "                    valid_paras = [p for p in paragraphs if len(p.get_text(strip=True)) > 50]\n",
    "                    \n",
    "                    return self.build_article(\n",
    "                        url=url,\n",
    "                        title=title,\n",
    "                        content=self.clean_text(valid_paras)\n",
    "                    )\n",
    "            \n",
    "            self.scrapers[domain] = DynamicScraper(source_name, domain)\n",
    "        \n",
    "        return self.scrapers[domain]\n",
    "\n",
    "    def collect_articles(self, max_articles=20, days=7):\n",
    "        \"\"\"\n",
    "        Recoge artículos de todas las fuentes.\n",
    "        Devuelve lista de dicts con: source, url, title, content\n",
    "        \"\"\"\n",
    "        from datetime import datetime, timedelta\n",
    "        import time\n",
    "        \n",
    "        all_articles = []\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        \n",
    "        for source in self.sources:\n",
    "            logging.info(f\"Procesando {source['name']}...\")\n",
    "            feed = feedparser.parse(source[\"rss\"])\n",
    "            \n",
    "            for entry in feed.entries:\n",
    "                # Filtrar por fecha si es posible\n",
    "                pub_str = entry.get('published', entry.get('updated', ''))\n",
    "                try:\n",
    "                    pub_date = datetime.strptime(pub_str, '%a, %d %b %Y %H:%M:%S %z')\n",
    "                    pub_date = pub_date.replace(tzinfo=None)\n",
    "                    if pub_date < cutoff:\n",
    "                        continue\n",
    "                except:\n",
    "                    pass  # si no hay fecha, lo incluimos\n",
    "                \n",
    "                url = entry.get('link', '')\n",
    "                title = entry.get('title', '')\n",
    "                \n",
    "                if not url or not title:\n",
    "                    continue\n",
    "                \n",
    "                # Obtener scraper y scrapear contenido\n",
    "                scraper = self._get_scraper(source['name'], url)\n",
    "                article = scraper.scrape_article(url)\n",
    "                \n",
    "                if article:\n",
    "                    all_articles.append(article)\n",
    "                    logging.info(f\"  ✓ {title[:60]}...\")\n",
    "                else:\n",
    "                    logging.warning(f\"  ✗ Falló: {title[:60]}...\")\n",
    "                \n",
    "                # Delay entre artículos para no saturar\n",
    "                time.sleep(2)\n",
    "                \n",
    "                if len(all_articles) >= max_articles:\n",
    "                    break\n",
    "            \n",
    "            if len(all_articles) >= max_articles:\n",
    "                break\n",
    "        \n",
    "        return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "415d2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Procesando OpenAI...\n",
      "INFO:root:Procesando DeepMind...\n",
      "WARNING:root:  ✗ Falló: Gemini 3 Deep Think: Advancing science, research and enginee...\n",
      "WARNING:root:  ✗ Falló: Accelerating Mathematical and Scientific Discovery with Gemi...\n",
      "INFO:root:Procesando Anthropic...\n",
      "INFO:root:Procesando MIT AI...\n",
      "INFO:root:  ✓ New J-PAL research and policy initiative to test and scale A...\n",
      "INFO:root:  ✓ Accelerating science with AI and simulations...\n",
      "INFO:root:  ✓ Using synthetic biology and AI to address global antimicrobi...\n",
      "INFO:root:  ✓ AI algorithm enables tracking of vital white matter pathways...\n",
      "INFO:root:  ✓ 3 Questions: Using AI to help Olympic skaters land a quint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total artículos obtenidos: 5\n",
      "\n",
      "Ejemplo del primer artículo:\n",
      "  source: MIT AI\n",
      "  url: https://news.mit.edu/2026/new-j-pal-research-policy-initiative-to-test-scale-ai-innovations-fight-poverty-0212\n",
      "  title: New J-PAL research and policy initiative to test and scale AI innovations to fight poverty\n",
      "  content: The Abdul Latif Jameel Poverty Action Lab (J-PAL) at MIT has awarded funding to eight new research studies to understand how artificial intelligence innovations can be used in the fight against povert...\n",
      "  scraping_date: 2026-02-16 12:10:02.621821\n"
     ]
    }
   ],
   "source": [
    "collector = RSSContentCollector2()\n",
    "\n",
    "# Obtener máximo 15 artículos (ajusta según necesites)\n",
    "articulos = collector.collect_articles(max_articles=300, days=7)\n",
    "\n",
    "print(f\"\\nTotal artículos obtenidos: {len(articulos)}\")\n",
    "\n",
    "# Mostrar ejemplo del primero\n",
    "if articulos:\n",
    "    print(\"\\nEjemplo del primer artículo:\")\n",
    "    for key, value in articulos[0].items():\n",
    "        if key == 'content':\n",
    "            print(f\"  {key}: {value[:200]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f478cac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articulos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
