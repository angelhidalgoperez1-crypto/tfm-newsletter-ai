{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd397238",
   "metadata": {},
   "source": [
    "# AI-powered Newsletter Generator\n",
    "**TFM ‚Äì End-to-end pipeline**\n",
    "\n",
    "Este notebook implementa un pipeline completo para:\n",
    "- Agregar noticias tecnol√≥gicas de m√∫ltiples fuentes\n",
    "- Representarlas sem√°nticamente mediante embeddings\n",
    "- Agruparlas autom√°ticamente por √°reas tem√°ticas\n",
    "- Curarlas y priorizarlas para la generaci√≥n de una newsletter\n",
    "\n",
    "El foco no est√° solo en la parte t√©cnica, sino en justificar cada decisi√≥n\n",
    "desde un punto de vista de **valor para negocio**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486691d",
   "metadata": {},
   "source": [
    "# 1. Libraries and other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9765b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = r\"C:\\Users\\Angel\\OneDrive - Universidad Complutense de Madrid (UCM)\\Documentos\\MASTER\\99_tfm\\tfm_newsletter_ai\"\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from config.paths import (\n",
    "    RAW_DATA_DIR,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    DIAGNOSTICS_DIR,\n",
    "    NEWSLETTER_DIR,\n",
    "    MODEL_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d821571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from config.load_config import load_config\n",
    "from sklearn.cluster import KMeans, HDBSCAN\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Scraping imports\n",
    "from scraping.scraper_base import BaseScraper\n",
    "from scraping.normalization import normalize_article\n",
    "from scraping.sources.scraper_xataka import XatakaScraper\n",
    "from scraping.sources.scraper_huggingface import HuggingFaceScraper\n",
    "from scraping.sources.scraper_techcrunch import TechCrunchScraper\n",
    "from scraping.sources.scraper_aws import AWSScraper\n",
    "from scraping.sources.scraper_wired import WiredScraper\n",
    "from scraping.sources.scraper_microsoft import MicrosoftNewsScraper\n",
    "from scraping.sources.scraper_aibusiness import AIBusinessScraper\n",
    "from scraping.sources.scraper_openai import OpenAIScraper\n",
    "\n",
    "# NLP imports\n",
    "from nlp.preprocessing import basic_preprocess\n",
    "from nlp.embeddings import SentenceTransformerEmbedder\n",
    "from nlp.cleaning_tfidf import clean_for_tfidf, compute_tfidf\n",
    "from nlp.clustering import fit_kmeans, find_optimal_k, compute_similarity_to_centroid\n",
    "from nlp.interpretation import top_terms_per_cluster, name_clusters\n",
    "from nlp.scoring import compute_source_score, compute_novelty_scores, compute_recency_score, compute_final_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20afeb1c",
   "metadata": {},
   "source": [
    "# 2. Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa51f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7678, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scrapers = [\n",
    "    XatakaScraper(),\n",
    "    HuggingFaceScraper(max_pages=50, sleep_time=2),\n",
    "    TechCrunchScraper(max_pages=50),\n",
    "    AWSScraper(max_pages=50,\n",
    "               blogs=[\"machine-learning\",\n",
    "                    \"infrastructure-and-automation\",\n",
    "                    \"iot\",\n",
    "                    \"big-data\"\n",
    "                    ]\n",
    "            ),\n",
    "    WiredScraper(max_pages=50)\n",
    "    # # MicrosoftNewsScraper(),\n",
    "    # # AIBusinessScraper(),\n",
    "    # OpenAIScraper()\n",
    "]\n",
    "\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "articles = []\n",
    "\n",
    "for scraper in scrapers:\n",
    "    links = scraper.get_article_links()\n",
    "    for url in links:\n",
    "        article = scraper.scrape_article(url)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "        sleep(1+random.uniform(0, 3))\n",
    "\n",
    "len(links)\n",
    "\n",
    "normalized_articles = [normalize_article(article) for article in articles]\n",
    "\n",
    "df = pd.DataFrame(normalized_articles)\n",
    "\n",
    "df_clean = df[df[\"is_valid\"]].copy()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "df_clean.to_csv(os.path.join(RAW_DATA_DIR, \"definite_articles.csv\"), index=False, sep=\";\")\n",
    "df_clean.to_pickle(os.path.join(RAW_DATA_DIR, \"definite_articles.pkl\"))\n",
    "df_clean.to_parquet(os.path.join(RAW_DATA_DIR, \"definite_articles.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eadb8dd",
   "metadata": {},
   "source": [
    "# 3. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68debd",
   "metadata": {},
   "source": [
    "## 3.1 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dd22c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce552089bdcb456e9d90c7e9f86f1f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/distiluse-base-multilingual-cased-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab60c0d6950452e8422c307f95bf3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0c52951b2f498c93e9e208b1961f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'df_clean' not in locals() or df_clean.empty:\n",
    "    df_clean = pd.read_pickle(os.path.join(RAW_DATA_DIR, \"definite_articles.pkl\"))\n",
    "\n",
    "df_clean[\"text_for_embedding\"] = (\n",
    "    df_clean[\"title\"] + \". \" + df_clean[\"content\"]\n",
    ").apply(basic_preprocess)\n",
    "\n",
    "models_to_test = [\n",
    "    \"miniLM_multilingual\",\n",
    "    \"distiluse_multilingual\",\n",
    "    \"mpnet_en\"\n",
    "]\n",
    "\n",
    "embeddings_by_model = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    embedder = SentenceTransformerEmbedder(model_name)\n",
    "    embeddings = embedder.encode(df_clean[\"text_for_embedding\"].tolist())\n",
    "    embeddings_by_model[model_name] = embeddings\n",
    "    df_clean[f\"embedding_{model_name}\"] = embeddings.tolist()\n",
    "\n",
    "df_clean.to_parquet(os.path.join(PROCESSED_DATA_DIR, \"definite_articles_with_embeddings.parquet\"))\n",
    "df_clean.to_pickle(os.path.join(PROCESSED_DATA_DIR, \"definite_articles_with_embeddings.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4d2ae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k for miniLM_multilingual: 5\n",
      "Optimal k for distiluse_multilingual: 5\n",
      "Optimal k for mpnet_en: 4\n"
     ]
    }
   ],
   "source": [
    "if 'df_clean' not in locals() or df_clean.empty:\n",
    "    df_clean = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, \"definite_articles_with_embeddings.pkl\"))\n",
    "\n",
    "df_no_wired = df_clean[~df_clean[\"source\"].str.contains(\"Wired\", case=False, na=False)].copy()\n",
    "\n",
    "# Create embeddings for each model\n",
    "models_names = [\"miniLM_multilingual\", \"distiluse_multilingual\", \"mpnet_en\"]\n",
    "var_emb1 = np.vstack(df_no_wired[f\"embedding_{models_names[0]}\"].values)\n",
    "var_emb2 = np.vstack(df_no_wired[f\"embedding_{models_names[1]}\"].values)\n",
    "var_emb3 = np.vstack(df_no_wired[f\"embedding_{models_names[2]}\"].values)\n",
    "\n",
    "# Find optimal k for each model\n",
    "optimal_k1, scores1 = find_optimal_k(var_emb1, k_min=4, k_max=12)\n",
    "optimal_k2, scores2 = find_optimal_k(var_emb2, k_min=4, k_max=12)\n",
    "optimal_k3, scores3 = find_optimal_k(var_emb3, k_min=4, k_max=12)\n",
    "\n",
    "print(f\"Optimal k for {models_names[0]}: {optimal_k1}\")\n",
    "print(f\"Optimal k for {models_names[1]}: {optimal_k2}\")\n",
    "print(f\"Optimal k for {models_names[2]}: {optimal_k3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d78b6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({4: np.float64(0.06697685694394702),\n",
       "  5: np.float64(0.08959622504079924),\n",
       "  6: np.float64(0.07579938788502158),\n",
       "  7: np.float64(0.08477468668931956),\n",
       "  8: np.float64(0.08581159379752108),\n",
       "  9: np.float64(0.08026068897747898),\n",
       "  10: np.float64(0.06099539506644597),\n",
       "  11: np.float64(0.07109295975101365),\n",
       "  12: np.float64(0.06316784819323495)},\n",
       " {4: np.float64(0.3292829678608477),\n",
       "  5: np.float64(0.33068088213379837),\n",
       "  6: np.float64(0.05648172617268138),\n",
       "  7: np.float64(0.05422970686438908),\n",
       "  8: np.float64(0.05420400658382348),\n",
       "  9: np.float64(0.0581838171757661),\n",
       "  10: np.float64(0.05403503002803746),\n",
       "  11: np.float64(0.056382463566916244),\n",
       "  12: np.float64(0.05567519347729249)},\n",
       " {4: np.float64(0.13428056520618917),\n",
       "  5: np.float64(0.10852472425648775),\n",
       "  6: np.float64(0.13424625344534427),\n",
       "  7: np.float64(0.11699992347266841),\n",
       "  8: np.float64(0.11322286815076396),\n",
       "  9: np.float64(0.10913348864307991),\n",
       "  10: np.float64(0.10923094818766808),\n",
       "  11: np.float64(0.10748423482756943),\n",
       "  12: np.float64(0.10556048175841967)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1, scores2, scores3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be145a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KMeans for each model\n",
    "kmeans1 = fit_kmeans(var_emb1, k=optimal_k1)\n",
    "kmeans2 = fit_kmeans(var_emb2, k=optimal_k2)\n",
    "kmeans3 = fit_kmeans(var_emb3, k=optimal_k3)\n",
    "\n",
    "# Add cluster assignments for each model (using the first model for primary clustering)\n",
    "df_no_wired[\"cluster\"] = kmeans1[1]\n",
    "df_no_wired[\"cluster_model2\"] = kmeans2[1]\n",
    "df_no_wired[\"cluster_model3\"] = kmeans3[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be7001",
   "metadata": {},
   "source": [
    "## 3.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_no_wired' not in locals() or df_no_wired.empty:\n",
    "    df_no_wired = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, \"definite_articles_with_embeddings.pkl\"))\n",
    "    df_no_wired = df_no_wired[~df_no_wired[\"source\"].str.contains(\"Wired\", case=False, na=False)].copy()\n",
    "\n",
    "if 'var_emb1' not in locals():\n",
    "    models_names = [\"miniLM_multilingual\", \"distiluse_multilingual\", \"mpnet_en\"]\n",
    "    var_emb1 = np.vstack(df_no_wired[f\"embedding_{models_names[0]}\"].values)\n",
    "    var_emb2 = np.vstack(df_no_wired[f\"embedding_{models_names[1]}\"].values)\n",
    "    var_emb3 = np.vstack(df_no_wired[f\"embedding_{models_names[2]}\"].values)\n",
    "\n",
    "df_no_wired[\"text_tfidf\"] = df_no_wired.apply(\n",
    "    lambda row: clean_for_tfidf(row[\"content\"], row[\"language\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "X_tfidf, tfidf_vectorizer = compute_tfidf(df_no_wired[\"text_tfidf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e14cf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_terms = top_terms_per_cluster(\n",
    "    X_tfidf,\n",
    "    df_no_wired[\"cluster\"].values,\n",
    "    tfidf_vectorizer ,\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "cluster_names = name_clusters(cluster_terms)\n",
    "df_no_wired[\"cluster_name\"] = df_no_wired[\"cluster\"].map(cluster_names)\n",
    "\n",
    "df_no_wired.to_pickle(os.path.join(PROCESSED_DATA_DIR, \"definite_articles_clustered.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b8353",
   "metadata": {},
   "source": [
    "## 3.3 Duplicates and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39398197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_no_wired.copy()\n",
    "\n",
    "models_names = [\"miniLM_multilingual\", \"distiluse_multilingual\", \"mpnet_en\"]\n",
    "# Create embeddings for each model\n",
    "var_emb1 = np.vstack(df_processed[f\"embedding_{models_names[0]}\"].values)\n",
    "var_emb2 = np.vstack(df_processed[f\"embedding_{models_names[1]}\"].values)\n",
    "var_emb3 = np.vstack(df_processed[f\"embedding_{models_names[2]}\"].values)\n",
    "\n",
    "# Create similarity matrices for each model\n",
    "similarity_matrix1 = cosine_similarity(var_emb1)\n",
    "similarity_matrix2 = cosine_similarity(var_emb2)\n",
    "similarity_matrix3 = cosine_similarity(var_emb3)\n",
    "\n",
    "# Average similarity matrix from all models\n",
    "similarity_matrix = (similarity_matrix1 + similarity_matrix2 + similarity_matrix3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efa08a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6485"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2490edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_THRESHOLD = 0.85\n",
    "\n",
    "duplicates = []\n",
    "n = similarity_matrix.shape[0]\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        if similarity_matrix[i, j] > SIM_THRESHOLD:\n",
    "            duplicates.append((i, j, similarity_matrix[i, j]))\n",
    "\n",
    "to_drop = set()\n",
    "\n",
    "indices_validos = set(df_processed.index)\n",
    "\n",
    "for i, j, sim in duplicates:\n",
    "    if i in indices_validos and j in indices_validos:\n",
    "        if df_processed.loc[i, \"word_count\"] >= df_processed.loc[j, \"word_count\"]:\n",
    "            to_drop.add(j)\n",
    "        else:\n",
    "            to_drop.add(i)\n",
    "\n",
    "df_curated = df_processed.drop(index=to_drop).reset_index(drop=True)\n",
    "\n",
    "models_names = [\"miniLM_multilingual\", \"distiluse_multilingual\", \"mpnet_en\"]\n",
    "# Recreate embeddings for curated dataset\n",
    "var_emb1 = np.vstack(df_curated[f\"embedding_{models_names[0]}\"].values)\n",
    "var_emb2 = np.vstack(df_curated[f\"embedding_{models_names[1]}\"].values)\n",
    "var_emb3 = np.vstack(df_curated[f\"embedding_{models_names[2]}\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d0c0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scores using all three models\n",
    "# novelty_score1 = compute_novelty_scores(var_emb1, df_curated[\"cluster\"].values)\n",
    "novelty_score2 = compute_novelty_scores(var_emb2, df_curated[\"cluster_model2\"].values)\n",
    "novelty_score3 = compute_novelty_scores(var_emb3, df_curated[\"cluster_model3\"].values)\n",
    "\n",
    "# Average novelty score from all models\n",
    "df_curated[\"novelty_score\"] = (novelty_score2)\n",
    "df_curated[\"recency_score\"] = compute_recency_score(df_curated)\n",
    "df_curated[\"source_score\"] = compute_source_score(df_curated)\n",
    "\n",
    "# Compute similarity to centroid for each model\n",
    "# sim_to_centroid1 = compute_similarity_to_centroid(var_emb1, df_curated[\"cluster\"].values, kmeans1[2])\n",
    "sim_to_centroid2 = compute_similarity_to_centroid(var_emb2, df_curated[\"cluster_model2\"].values, kmeans2[2])\n",
    "sim_to_centroid3 = compute_similarity_to_centroid(var_emb3, df_curated[\"cluster_model3\"].values, kmeans3[2])\n",
    "\n",
    "# Average similarity to centroid\n",
    "df_curated[\"similarity_to_centroid\"] = (sim_to_centroid2)\n",
    "\n",
    "df_scored = compute_final_score(df_curated,\n",
    "                                w_similarity=0.4,\n",
    "                                w_novelty=0.3,\n",
    "                                w_recency=0.2,\n",
    "                                w_source=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c2d86",
   "metadata": {},
   "source": [
    "## 3.4 Adjusting to AMC Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b05c4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMC_PROFILES = {\n",
    "    \"Data & AI\": \"machine learning artificial intelligence data science models analytics\",\n",
    "    \"IT & Cloud\": \"cloud infrastructure devops automation security networks azure aws\",\n",
    "    \"Business\": \"strategy innovation digital transformation operations performance\",\n",
    "    \"Risk & Compliance\": \"regulation ai ethics governance risk compliance privacy\",\n",
    "    \"Marketing\": \"digital marketing customer engagement personalization content automation advertising social media\",\n",
    "    \"Category Management\": \"product portfolio pricing market trends competition consumer insights procurement\",\n",
    "    \"Supply Chain\": \"logistics operations forecasting demand planning inventory optimization manufacturing\",\n",
    "    \"Sales / Commercial\": \"sales enablement customer acquisition crm revenue optimization commercial strategy\",\n",
    "    \"Board\": \"executive leadership corporate strategy decision making innovation governance investment\",\n",
    "    \"New Product Development\": \"product design innovation prototyping research development user needs experimentation\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06adbd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/distiluse-base-multilingual-cased-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb8c34f3e794910a9bee19ab1843426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e5495e4b3c42ecab7c0972a9891db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3ce7390e6e47a79379d84680bb6373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_names = [\"miniLM_multilingual\", \"distiluse_multilingual\", \"mpnet_en\"]\n",
    "# Get embeddings for scored articles\n",
    "var_emb1 = np.vstack(df_scored[f\"embedding_{models_names[0]}\"].values)\n",
    "var_emb2 = np.vstack(df_scored[f\"embedding_{models_names[1]}\"].values)\n",
    "var_emb3 = np.vstack(df_scored[f\"embedding_{models_names[2]}\"].values)\n",
    "\n",
    "# Create profile embeddings for each model\n",
    "embedder1 = SentenceTransformerEmbedder(models_names[0])\n",
    "embedder2 = SentenceTransformerEmbedder(models_names[1])\n",
    "embedder3 = SentenceTransformerEmbedder(models_names[2])\n",
    "\n",
    "profile_embs1 = embedder1.encode(list(AMC_PROFILES.values()))\n",
    "profile_embs2 = embedder2.encode(list(AMC_PROFILES.values()))\n",
    "profile_embs3 = embedder3.encode(list(AMC_PROFILES.values()))\n",
    "\n",
    "def assign_area(article_emb1, article_emb2, article_emb3, profile_embs1, profile_embs2, profile_embs3, areas):\n",
    "    sims1 = cosine_similarity([article_emb1], profile_embs1)[0]\n",
    "    sims2 = cosine_similarity([article_emb2], profile_embs2)[0]\n",
    "    sims3 = cosine_similarity([article_emb3], profile_embs3)[0]\n",
    "    # Average similarities from all models\n",
    "    avg_sims = (sims2)\n",
    "    return areas[np.argmax(avg_sims)]\n",
    "\n",
    "areas = list(AMC_PROFILES.keys())\n",
    "df_scored[\"area\"] = [\n",
    "    assign_area(var_emb1[i], var_emb2[i], var_emb3[i], profile_embs1, profile_embs2, profile_embs3, areas)\n",
    "    for i in range(len(df_scored))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275c58f",
   "metadata": {},
   "source": [
    "# 4. HTML - Nwesletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72e2c7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Data & AI</h1><ul>\n",
       "        <li>\n",
       "            <b>Distributed Training: Train BART/T5 for Summarization using ü§ó Transformers and Amazon SageMaker</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/sagemaker-distributed-training-seq2seq\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Announcing New Hugging Face and KerasHub integration</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/keras-hub-integration\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Deep Learning over the Internet: Training Language Models Collaboratively</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/collaborative-training\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì IT & Cloud</h1><ul>\n",
       "        <li>\n",
       "            <b>Introducing the Private Hub: A New Way to Build With Machine Learning</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/introducing-private-hub\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Groq on Hugging Face Inference Providers üî•</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/inference-providers-groq\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Perceiver IO: a scalable, fully-attentional model that works on any modality</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/perceiver\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Business</h1><ul>\n",
       "        <li>\n",
       "            <b>Optimum-NVIDIA on Hugging Face enables blazingly fast LLM inference in just 1 line of code</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/optimum-nvidia\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Making sense of this mess</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/transformers-docs-redesign\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Retrieval Augmented Generation with Huggingface Transformers and Ray</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/ray-rag\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Risk & Compliance</h1><ul>\n",
       "        <li>\n",
       "            <b>Democratizing AI Safety with RiskRubric.ai</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/riskrubric\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Ethics and Society Newsletter #1</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/ethics-soc-1\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Ryght‚Äôs Journey to Empower Healthcare and Life Sciences with Expert Support from Hugging Face</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/ryght-case-study\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Marketing</h1><ul>\n",
       "        <li>\n",
       "            <b>Improving Hugging Face Model Access for Kaggle Users</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/kaggle-integration\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Public AI on Hugging Face Inference Providers üî•</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/inference-providers-publicai\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>What is AutoRound?</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/autoround\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Category Management</h1><ul>\n",
       "        <li>\n",
       "            <b>Going multimodal: How Prezi is leveraging the Hub and the Expert Support Program to accelerate their ML roadmap</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/prezi-case-study\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Bringing the Artificial Analysis LLM Performance Leaderboard to Hugging Face</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/leaderboard-artificial-analysis\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Image Similarity with Hugging Face Datasets and Transformers</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/image-similarity\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Supply Chain</h1><ul>\n",
       "        <li>\n",
       "            <b>Hugging Face on AMD Instinct MI300 GPU</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/huggingface-amd-mi300\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Llama 2 on Amazon SageMaker a Benchmark</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/llama-sagemaker-benchmark\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/trl-peft\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Sales / Commercial</h1><ul>\n",
       "        <li>\n",
       "            <b>Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/optimum-onnxruntime-training\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Rocket Money x Hugging Face: Scaling Volatile ML Models in Production</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/rocketmoney-case-study\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Cohere on Hugging Face Inference Providers üî•</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/inference-providers-cohere\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì Board</h1><ul>\n",
       "        <li>\n",
       "            <b>A guide to setting up your own Hugging Face leaderboard: an end-to-end example with Vectara's hallucination leaderboard</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/leaderboard-vectara\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>How Hugging Face Accelerated Development of Witty Works Writing Assistant</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/classification-use-cases\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Introducing the Open Leaderboard for Hebrew LLMs!</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/leaderboard-hebrew\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Newsletter ‚Äì New Product Development</h1><ul>\n",
       "        <li>\n",
       "            <b>Welcome GPT OSS, the new open-source model family from OpenAI!</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/welcome-openai-gpt-oss\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Introducing multi-backends (TRT-LLM, vLLM) support for Text Generation Inference</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/tgi-multi-backend\">Leer m√°s</a>\n",
       "        </li>\n",
       "        \n",
       "        <li>\n",
       "            <b>Text-Generation Pipeline on Intel¬Æ Gaudi¬Æ 2 AI Accelerator</b><br>\n",
       "            <i>Hugging Face Blog</i><br>\n",
       "            <a href=\"https://huggingface.co/blog/textgen-pipe-gaudi\">Leer m√°s</a>\n",
       "        </li>\n",
       "        </ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_newsletter(df_area, area_name):\n",
    "    html = f\"<h1>Newsletter ‚Äì {area_name}</h1><ul>\"\n",
    "    for _, row in df_area.sort_values(\"final_score\", ascending=False).head(3).iterrows():\n",
    "        html += f\"\"\"\n",
    "        <li>\n",
    "            <b>{row['title']}</b><br>\n",
    "            <i>{row['source']}</i><br>\n",
    "            <a href=\"{row['url']}\">Leer m√°s</a>\n",
    "        </li>\n",
    "        \"\"\"\n",
    "    html += \"</ul>\"\n",
    "    return html\n",
    "\n",
    "for area in AMC_PROFILES.keys():\n",
    "    # df_area = df_scored[df_scored[\"area\"] == area]\n",
    "    # newsletter_html = generate_newsletter(df_area, area)\n",
    "    # with open(os.path.join(NEWSLETTER_DIR, f\"newsletter_{area.replace(' ', '_')}.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    #     f.write(newsletter_html)\n",
    "    display(HTML(generate_newsletter(df_scored[df_scored[\"area\"] == area], area)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817121",
   "metadata": {},
   "source": [
    "# 5. Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8364d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos guardados correctamente\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "models_names = [\"miniLM_multilingual\", \"distiluse_multilingual\", \"mpnet_en\"]\n",
    "\n",
    "# Saving KMeans for all three models\n",
    "joblib.dump(kmeans1[0], os.path.join(MODEL_DIR, \"kmeans_model1.pkl\"))\n",
    "joblib.dump(kmeans2[0], os.path.join(MODEL_DIR, \"kmeans_model2.pkl\"))\n",
    "joblib.dump(kmeans3[0], os.path.join(MODEL_DIR, \"kmeans_model3.pkl\"))\n",
    "\n",
    "# Saving cluster metadata for each model\n",
    "for idx, (kmeans, model_name) in enumerate([(kmeans1, models_names[0]), (kmeans2, models_names[1]), (kmeans3, models_names[2])], 1):\n",
    "    cluster_metadata = {\n",
    "        \"model_name\": model_name,\n",
    "        \"n_clusters\": kmeans[0].n_clusters,\n",
    "        \"inertia\": kmeans[0].inertia_,\n",
    "        \"centroids\": kmeans[0].cluster_centers_.tolist()\n",
    "    }\n",
    "    with open(os.path.join(MODEL_DIR, f\"cluster_metadata_model{idx}.json\"), \"w\") as f:\n",
    "        json.dump(cluster_metadata, f, indent=2)\n",
    "\n",
    "# Saving processed_urls\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, \"processed_urls.json\"), \"w\") as f:\n",
    "    json.dump(links, f)\n",
    "\n",
    "print(\"Modelos guardados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4664d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6485, 16), (6120, 22), (6120, 22))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_wired.shape, df_curated.shape, df_scored.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
